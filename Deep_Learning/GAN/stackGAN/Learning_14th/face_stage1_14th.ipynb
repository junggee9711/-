{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rapid-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings:  (14261, 40)\n",
      "Embeddings shape: (14261, 40)\n",
      "embeddings:  (3565, 40)\n",
      "Embeddings shape: (3565, 40)\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
    "    concatenate, Flatten, Lambda, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "   \n",
    "def load_class_ids(class_info_file_path):\n",
    "    \"\"\"\n",
    "    Load class ids from class_info.pickle file\n",
    "    \"\"\"\n",
    "    with open(class_info_file_path, 'rb') as f:\n",
    "        class_ids = pickle.load(f, encoding='latin1')\n",
    "        return class_ids\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_file_path):\n",
    "    \"\"\"\n",
    "    훈련된 텍스트 임베딩을 불러옴\n",
    "    \"\"\"\n",
    "    with open(embeddings_file_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f, encoding='latin1')\n",
    "        embeddings = np.array(embeddings)\n",
    "        print('embeddings: ', embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "def load_filenames(filenames_file_path):\n",
    "    \"\"\"\n",
    "    Load filenames.pickle file and return a list of all file names\n",
    "    \"\"\"\n",
    "    with open(filenames_file_path, 'rb') as f:\n",
    "        filenames = pickle.load(f, encoding='latin1')\n",
    "    return filenames\n",
    "\n",
    "def load_bounding_boxes(dataset_dir):\n",
    "    \"\"\"\n",
    "    이미지와 그에 상응하는 바운딩 박스를 짝지어 딕셔너리로 만들어 출력\n",
    "    \"\"\"\n",
    "    # 바운딩 박스 전체 경로\n",
    "    bounding_boxes_path = os.path.join(dataset_dir, 'list_bbox_celeba_pure_del.csv')\n",
    "    file_paths_path = os.path.join(dataset_dir, 'list_filenames.csv')\n",
    "\n",
    "    # bounding_boxes.txt 와 images.txt 파일을 읽어옴\n",
    "    df_bounding_boxes = pd.read_csv(bounding_boxes_path, header=None).astype(int)\n",
    "    df_file_names = pd.read_csv(file_paths_path, header=None)\n",
    "\n",
    "    # 전체 이미지 파일 명이 순서대로 적힌 리스트를 만듬\n",
    "    file_names = df_file_names[0].tolist() \n",
    "\n",
    "    # 파일 이름에 대응하는 바운딩 박스가 들어갈 딕셔너리를 만듬 (딕셔너리는 크기를 임의로 증가시킬수 있으므로 초기 사이즈는 아무렇게나)\n",
    "    filename_boundingbox_dict = {}\n",
    "\n",
    "    # 이미지 파일과 그에 해당하는 바운딩 박스를 딕셔너리로 만듬 (key = 이미지 파일 이름)\n",
    "    for i in range(0, len(file_names)):\n",
    "        # Get the bounding box\n",
    "        bounding_box = df_bounding_boxes.iloc[i][:].tolist()\n",
    "        key = file_names[i][:-4] + '.jpg'\n",
    "        filename_boundingbox_dict[key] = bounding_box\n",
    "\n",
    "    return filename_boundingbox_dict\n",
    "'''\n",
    "새 이미지가 크롭핑 되어있지 않기 크롭하기 위한 바운딩 박스 좌표 값이 파일에 주어지며,\n",
    "그 파일을 토대로 이미지를 크로핑 한 후,\n",
    "크로핑된 모든 이미지를 지정한 이미지 크기 (image_size) 값으로 바꾼다\n",
    "'''\n",
    "def get_img(img_path, bbox, image_size):\n",
    "    \"\"\"\n",
    "    Load and resize image\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    width, height = img.size\n",
    "    if bbox is not None:\n",
    "        pass\n",
    "    '''\n",
    "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "        y1 = np.maximum(0, center_y - R)\n",
    "        y2 = np.minimum(height, center_y + R)\n",
    "        x1 = np.maximum(0, center_x - R)\n",
    "        x2 = np.minimum(width, center_x + R)\n",
    "        img = img.crop([x1, y1, x2, y2])\n",
    "    '''\n",
    "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_dataset(filenames_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    filenames = load_filenames(filenames_file_path)\n",
    "    '''\n",
    "    class_ids = load_class_ids(class_info_file_path)\n",
    "    '''\n",
    "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
    "    all_embeddings = load_embeddings(embeddings_file_path)\n",
    "\n",
    "    X, y, embeddings = [], [], []\n",
    "\n",
    "    print(\"Embeddings shape:\", all_embeddings.shape)\n",
    "\n",
    "    # 각 이미지에 해당하는 바운딩 박스 딕셔너리를 추출하여 get_img 함수로 크로핑되고 같은 크기로 바뀐 이미지를 \n",
    "    for index, filename in enumerate(filenames):\n",
    "        bounding_box = bounding_boxes[filename]\n",
    "\n",
    "        try:\n",
    "            # Load images\n",
    "            img_name = '{0}/images/{1}'.format(cub_dataset_dir, filename)\n",
    "            img = get_img(img_name, bounding_box, image_size)\n",
    "            '''\n",
    "            all_embeddings1 = all_embeddings[index, :, :]\n",
    "\n",
    "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
    "            '''\n",
    "            embedding = all_embeddings[index, :]\n",
    "            # X = 정제한 이미지 리스트\n",
    "            X.append(np.array(img))\n",
    "            '''\n",
    "            # y = 정제한 이미지 인덱스\n",
    "            y.append(class_ids[index])\n",
    "            '''\n",
    "            # embeddings = \n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return X, embeddings\n",
    "\n",
    "\n",
    "def generate_c(x):\n",
    "    mean = x[:, :128]\n",
    "    log_sigma = x[:, 128:]\n",
    "    stddev = K.exp(log_sigma)\n",
    "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
    "    c = stddev * epsilon + mean\n",
    "    return c\n",
    "\n",
    "\n",
    "def build_ca_model():\n",
    "    \"\"\"\n",
    "    (1024,)의 텍스트 인코더 신경망의 출력을 입력으로 받고 (256,) 의 텐서를 출력\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(1024,))\n",
    "    x = Dense(256)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    model = Model(inputs=[input_layer], outputs=[x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_embedding_compressor_model():\n",
    "    \"\"\"\n",
    "    입력 속성값 (40,) 을 (128,) 의 벡터로 확장하는 네트워크\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(40,))\n",
    "    x = Dense(10)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=[x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_stage1_generator():\n",
    "    \"\"\"\n",
    "    Stage-I 의 generator \n",
    "    *** 이 신경망 안에 CA 신경망과 생성기 신경망이 들어가 있다!!!! ***\n",
    "    그러므로, 입력으로 텍스트 임베딩 출력 (1024,)과 잡음 변수(100,) 을 받는다\n",
    "    \"\"\"\n",
    "    '''\n",
    "    input_layer = Input(shape=(1024,))\n",
    "    x = Dense(256)(input_layer)\n",
    "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    c = Lambda(generate_c)(mean_logsigma)\n",
    "\n",
    "    input_layer2 = Input(shape=(100,))\n",
    "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
    "    '''\n",
    "    # 텍스트 조건부 변수를 잡음 변수와 접합(concatenation) -> cGAN\n",
    "\n",
    "    input_layer = Input(shape=(40,))\n",
    "    x = Dense(10)(input_layer)\n",
    "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
    "    x = Dropout(0.0)(x)\n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = Activation(activation='tanh')(x)\n",
    "\n",
    "    stage1_gen = Model(inputs=input_layer, outputs=[x, mean_logsigma])\n",
    "    '''\n",
    "    stage - I gen 은 입력된 문장의 임베딩을 바탕으로 (+잡음 변수) 이미지를 생성 함 \n",
    "    '''\n",
    "    return stage1_gen\n",
    "\n",
    "\n",
    "def build_stage1_discriminator():\n",
    "    \"\"\"\n",
    "    Create a model which takes two inputs\n",
    "    1. One from above network\n",
    "    2. One from the embedding layer\n",
    "    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(64, 64, 3))\n",
    "\n",
    "    x = Conv2D(64, (4, 4),\n",
    "               padding='same', strides=2,\n",
    "               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    '''\n",
    "    실제 이미지에 해당하는 압축된 임베딩을 입력\n",
    "    '''\n",
    "    input_layer2 = Input(shape=(4, 4, 10))\n",
    "\n",
    "    '''\n",
    "    입력 이미지와 압축 텍스트 임베딩을 합침\n",
    "    '''\n",
    "    merged_input = concatenate([x, input_layer2])\n",
    "\n",
    "    x2 = Conv2D(64 * 8, kernel_size=1,\n",
    "                padding=\"same\", strides=1)(merged_input)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    x2 = Dense(1)(x2)\n",
    "    x2 = Activation('sigmoid')(x2)\n",
    "\n",
    "    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=[x2])\n",
    "    '''\n",
    "    출력은 입력 이미지가 진짜인지 가짜인지에 관한 확률(sigmoid)을 출력\n",
    "    '''\n",
    "    return stage1_dis\n",
    "\n",
    "\n",
    "def build_adversarial_model(gen_model, dis_model):\n",
    "    input_layer = Input(shape=(40,))\n",
    "    input_layer3 = Input(shape=(4, 4, 10))\n",
    "\n",
    "    x, mean_logsigma = gen_model(input_layer)\n",
    "\n",
    "    dis_model.trainable = False\n",
    "    valid = dis_model([x, input_layer3])\n",
    "\n",
    "    model = Model(inputs=[input_layer, input_layer3], outputs=[valid, mean_logsigma])\n",
    "    return model\n",
    "\n",
    "\n",
    "def KL_loss(y_true, y_pred):\n",
    "    mean = y_pred[:, :128]\n",
    "    logsigma = y_pred[:, :128]\n",
    "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
    "    loss = K.mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def custom_generator_loss(y_true, y_pred):\n",
    "    # Calculate binary cross entropy loss\n",
    "    return K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "'''\n",
    "def save_rgb_img(img, path):\n",
    "    \"\"\"\n",
    "    Save an rgb image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Image\")\n",
    "\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "'''\n",
    "def save_rgb_img(img_gen, img_real, path):\n",
    "    \"\"\"\n",
    "    Save an rgb image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.imshow(img_gen)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Image_gen\")\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.imshow(img_real)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Image_real\")\n",
    "    \n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "def write_log(callback, name, loss, batch_no):\n",
    "    \"\"\"\n",
    "    Write training summary to TensorBoard\n",
    "    \"\"\"\n",
    "    with callback.as_default():\n",
    "          tf.summary.scalar(name, loss, batch_no)\n",
    "          callback.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Stage - I stackGAN 훈련\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    하이퍼파라미터(불변 파라미터) 지정\n",
    "    '''\n",
    "    data_dir = \"/home/csle/Desktop/CelebA_dataset_reduce_cropped\"\n",
    "    train_dir = data_dir + \"/train\"\n",
    "    test_dir = data_dir + \"/test\"\n",
    "    image_size = 64\n",
    "    batch_size = 128\n",
    "    z_dim = 100\n",
    "    stage1_generator_lr = 0.0002\n",
    "    stage1_discriminator_lr = 0.0002\n",
    "    stage1_lr_decay_step = 600\n",
    "    epochs = 1500\n",
    "    condition_dim = 10\n",
    "\n",
    "    embeddings_file_path_train = train_dir + \"/attr_(embeddings).pickle\"\n",
    "    embeddings_file_path_test = test_dir + \"/attr_(embeddings).pickle\"\n",
    "\n",
    "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
    "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
    "\n",
    "    '''\n",
    "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
    "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
    "    '''\n",
    "\n",
    "    cub_dataset_dir = data_dir + \"/img_align_celeba\"\n",
    "\n",
    "    '''\n",
    "    optimizer 정의\n",
    "    '''\n",
    "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    \"\"\"\"\n",
    "    dataset 로드하기\n",
    "    \"\"\"\n",
    "    X_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
    "                                                      cub_dataset_dir=cub_dataset_dir,\n",
    "                                                      embeddings_file_path=embeddings_file_path_train,\n",
    "                                                      image_size=(64, 64))\n",
    "\n",
    "    X_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
    "                                                   cub_dataset_dir=cub_dataset_dir,\n",
    "                                                   embeddings_file_path=embeddings_file_path_test,\n",
    "                                                   image_size=(64, 64))\n",
    "    \n",
    "    print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "outdoor-decision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Epoch is: 0\n",
      "Number of batches 111\n",
      "Batch:1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e560eb0ab9e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# stage-I 의 gen 에서 텍스트 임베딩을 바탕으로 저 해상도 fake 이미지를 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# 이때, 두 stage 는 랜덤하게 생성한 텍스트 임베딩을 나머지 입력으로 받음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mfake_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage1_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# stage-I dis 에 들어갈 압축 텍스트 임베딩을 랜덤하게 생성한 텍스트 임베딩 기반으로 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#with tf.device('/gpu:0'): \n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6 \n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "신경망 빌드 & compile\n",
    "\"\"\"\n",
    "\n",
    "ca_model = build_ca_model()\n",
    "ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "stage1_dis = build_stage1_discriminator()\n",
    "stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "stage1_gen = build_stage1_generator()\n",
    "stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
    "\n",
    "embedding_compressor_model = build_embedding_compressor_model()\n",
    "embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "'''\n",
    "stage-I GAN 빌드& 컴파일\n",
    "이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "'''\n",
    "adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
    "adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
    "                          optimizer=gen_optimizer, metrics=None)\n",
    "\n",
    "\"\"\"\n",
    "tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
    "tensorboard.set_model(stage1_gen)\n",
    "tensorboard.set_model(stage1_dis)\n",
    "'''\n",
    "tensorboard.set_model(ca_model)\n",
    "'''\n",
    "tensorboard.set_model(embedding_compressor_model)\n",
    "\"\"\"\n",
    "# Generate an array containing real and fake values\n",
    "# Apply label smoothing as well\n",
    "real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
    "fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
    "'''\n",
    "매 epoch 마다 아래를 반복함\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    print(\"========================================\")\n",
    "    print(\"Epoch is:\", epoch)\n",
    "    print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
    "\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "\n",
    "    # Load data and train model\n",
    "    number_of_batches = int(X_train.shape[0] / batch_size)\n",
    "    for index in range(number_of_batches):\n",
    "        print(\"Batch:{}\".format(index+1))\n",
    "        '''\n",
    "        모델에 입력으로 들어갈 이미지와 텍스트 임베딩을 받아옴 (각 텍스트 임베딩은 각 이미지에 대응 됨)\n",
    "        '''\n",
    "        # 원래는 CA 의 출력이 stage-I 으로 들어가야 하지만 gen 안에 CA 가 있음\n",
    "        z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        # 배치 사이즈만큼 훈련(실제) 이미지를 추출\n",
    "        image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "        # 추출한 이미지에 대응하는 임베딩을 추출\n",
    "        embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
    "        # 이미지들을 정규화하여 값을 작게 만듬\n",
    "        image_batch = (image_batch - 127.5) / 127.5\n",
    "\n",
    "        # stage-I 의 gen 에서 텍스트 임베딩을 바탕으로 저 해상도 fake 이미지를 생성\n",
    "        # 이때, 두 stage 는 랜덤하게 생성한 텍스트 임베딩을 나머지 입력으로 받음\n",
    "        fake_images, _ = stage1_gen.predict(embedding_batch, verbose=3)\n",
    "\n",
    "        # stage-I dis 에 들어갈 압축 텍스트 임베딩을 랜덤하게 생성한 텍스트 임베딩 기반으로 생성\n",
    "        compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
    "        compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
    "        compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
    "        '''\n",
    "        discriminator 에서 입력 이미지가 CNN 을 통과한 결과와 속성값 (40,) 을 concatenate 해 주기 위해서는,\n",
    "        속성값 (40,) 를 compressor 네트워크를 통해 (128,) 로 확장하고,\n",
    "        reshape, tile 함수를 이용해 CNN 결과 (4, 4, 512) 와 (4, 4, :) 부분을 맞추어 주어야 한다.\n",
    "        '''\n",
    "\n",
    "        \"\"\"\n",
    "        stage-I dis 를 훈련함\n",
    "        \"\"\"\n",
    "        # 실제 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 1 (이미지가 진짜라는 의미) 로 하여,\n",
    "        # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n",
    "                                                  np.reshape(real_labels, (batch_size, 1)))\n",
    "        # gen 이 생성한 가짜 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "        # dis 가 가짜 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n",
    "                                                  np.reshape(fake_labels, (batch_size, 1)))\n",
    "        # 실제 이미지와 압축 텍스트 임베딩을 '어긋나게' 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "        # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
    "                                                   np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
    "        dis_loss_wrong2 = stage1_dis.train_on_batch([image_batch[:(batch_size - 2)], compressed_embedding[2:]],\n",
    "                                                   np.reshape(fake_labels[2:], (batch_size-2, 1)))\n",
    "        \n",
    "        d_loss = 0.5*np.add(dis_loss_wrong2, 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake)))\n",
    "        \n",
    "        print(\"d_loss_real:{}\".format(dis_loss_real))\n",
    "        print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
    "        print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
    "        print(\"d_loss_wrong2:{}\".format(dis_loss_wrong))\n",
    "        \n",
    "        print(\"d_loss:{}\".format(d_loss))\n",
    "\n",
    "        \"\"\"\n",
    "        stage-I GAN 을 훈련함\n",
    "        이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "        \"\"\"\n",
    "        g_loss = adversarial_model.train_on_batch([embedding_batch, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
    "        print(\"g_loss:{}\".format(g_loss))\n",
    "\n",
    "        dis_losses.append(d_loss)\n",
    "        gen_losses.append(g_loss)\n",
    "\n",
    "    \"\"\"\n",
    "    각 epoch 마다 Tensorboard 에 loss 의 전체 배치 평균값 저장\n",
    "    \"\"\"\n",
    "    writer = tf.summary.create_file_writer(\"logs/\".format(time.time()))\n",
    "    write_log(writer, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
    "    write_log(writer, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "    '''\n",
    "    tf.summary.scalar('discriminator_loss', np.mean(dis_losses), epoch)\n",
    "    tf.summary.scalar('generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "\n",
    "    tf.summary.scalar('discriminator_loss', np.mean(dis_losses))\n",
    "    tf.summary.scalar('generator_loss', np.mean(gen_losses[0]))\n",
    "    summary_op = tf.summary.merge()\n",
    "    summary_writer = tf.summary.FileWriter(\"logs/\".format(time.time()))\n",
    "    '''    \n",
    "    # 매 두번의 epoch 마다 이미지 gen & 이미지 저장\n",
    "    if epoch % 2 == 0:\n",
    "        # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "        z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        embedding_batch = embeddings_test[0:batch_size]\n",
    "        fake_images, _ = stage1_gen.predict_on_batch(embedding_batch)\n",
    "\n",
    "        # Save images\n",
    "        '''\n",
    "        for i, img in enumerate(fake_images[:5]):\n",
    "            save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))\n",
    "        '''\n",
    "        \n",
    "        for i in range(5):\n",
    "            img_gen = fake_images[i]\n",
    "            img_real = X_test[i]\n",
    "            save_rgb_img(img_gen, img_real, \"results/gen_{}_{}.png\".format(epoch, i))\n",
    "\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # Save models\n",
    "        stage1_gen.save_weights(\"weights/stage1_gen_epoch_{}.h5\".format(epoch))\n",
    "        stage1_dis.save_weights(\"weights/stage1_dis_epoch_{}.h5\".format(epoch))\n",
    "\n",
    "'''\n",
    "이제 훈련된 stage-I 의 generator 와 discriminator 을 얻음 (+ embedding_compressor) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-smart",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
