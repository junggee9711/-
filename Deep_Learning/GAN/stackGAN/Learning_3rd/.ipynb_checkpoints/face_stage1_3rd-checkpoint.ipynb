{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
    "    concatenate, Flatten, Lambda, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "with tf.device('/gpu:0'):\n",
    "    def load_class_ids(class_info_file_path):\n",
    "        \"\"\"\n",
    "        Load class ids from class_info.pickle file\n",
    "        \"\"\"\n",
    "        with open(class_info_file_path, 'rb') as f:\n",
    "            class_ids = pickle.load(f, encoding='latin1')\n",
    "            return class_ids\n",
    "\n",
    "\n",
    "    def load_embeddings(embeddings_file_path):\n",
    "        \"\"\"\n",
    "        훈련된 텍스트 임베딩을 불러옴\n",
    "        \"\"\"\n",
    "        with open(embeddings_file_path, 'rb') as f:\n",
    "            embeddings = pickle.load(f, encoding='latin1')\n",
    "            embeddings = np.array(embeddings)\n",
    "            print('embeddings: ', embeddings.shape)\n",
    "        return embeddings\n",
    "\n",
    "    def load_filenames(filenames_file_path):\n",
    "        \"\"\"\n",
    "        Load filenames.pickle file and return a list of all file names\n",
    "        \"\"\"\n",
    "        with open(filenames_file_path, 'rb') as f:\n",
    "            filenames = pickle.load(f, encoding='latin1')\n",
    "        return filenames\n",
    "\n",
    "    def load_bounding_boxes(dataset_dir):\n",
    "        \"\"\"\n",
    "        이미지와 그에 상응하는 바운딩 박스를 짝지어 딕셔너리로 만들어 출력\n",
    "        \"\"\"\n",
    "        # 바운딩 박스 전체 경로\n",
    "        bounding_boxes_path = os.path.join(dataset_dir, 'list_bbox_celeba_pure.csv')\n",
    "        file_paths_path = os.path.join(dataset_dir, 'list_filenames.csv')\n",
    "\n",
    "        # bounding_boxes.txt 와 images.txt 파일을 읽어옴\n",
    "        df_bounding_boxes = pd.read_csv(bounding_boxes_path, header=None).astype(int)\n",
    "        df_file_names = pd.read_csv(file_paths_path, header=None)\n",
    "\n",
    "        # 전체 이미지 파일 명이 순서대로 적힌 리스트를 만듬\n",
    "        file_names = df_file_names[0].tolist() \n",
    "\n",
    "        # 파일 이름에 대응하는 바운딩 박스가 들어갈 딕셔너리를 만듬 (딕셔너리는 크기를 임의로 증가시킬수 있으므로 초기 사이즈는 아무렇게나)\n",
    "        filename_boundingbox_dict = {}\n",
    "\n",
    "        # 이미지 파일과 그에 해당하는 바운딩 박스를 딕셔너리로 만듬 (key = 이미지 파일 이름)\n",
    "        for i in range(0, len(file_names)):\n",
    "            # Get the bounding box\n",
    "            bounding_box = df_bounding_boxes.iloc[i][:].tolist()\n",
    "            key = file_names[i][:-4] + '.jpg'\n",
    "            filename_boundingbox_dict[key] = bounding_box\n",
    "\n",
    "        return filename_boundingbox_dict\n",
    "    '''\n",
    "    새 이미지가 크롭핑 되어있지 않기 크롭하기 위한 바운딩 박스 좌표 값이 파일에 주어지며,\n",
    "    그 파일을 토대로 이미지를 크로핑 한 후,\n",
    "    크로핑된 모든 이미지를 지정한 이미지 크기 (image_size) 값으로 바꾼다\n",
    "    '''\n",
    "    def get_img(img_path, bbox, image_size):\n",
    "        \"\"\"\n",
    "        Load and resize image\n",
    "        \"\"\"\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        width, height = img.size\n",
    "        if bbox is not None:\n",
    "            R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "            y1 = np.maximum(0, center_y - R)\n",
    "            y2 = np.minimum(height, center_y + R)\n",
    "            x1 = np.maximum(0, center_x - R)\n",
    "            x2 = np.minimum(width, center_x + R)\n",
    "            img = img.crop([x1, y1, x2, y2])\n",
    "        img = img.resize(image_size, PIL.Image.BILINEAR)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def load_dataset(filenames_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
    "        \"\"\"\n",
    "        Load dataset\n",
    "        \"\"\"\n",
    "        filenames = load_filenames(filenames_file_path)\n",
    "        '''\n",
    "        class_ids = load_class_ids(class_info_file_path)\n",
    "        '''\n",
    "        bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
    "        all_embeddings = load_embeddings(embeddings_file_path)\n",
    "\n",
    "        X, y, embeddings = [], [], []\n",
    "\n",
    "        print(\"Embeddings shape:\", all_embeddings.shape)\n",
    "\n",
    "        # 각 이미지에 해당하는 바운딩 박스 딕셔너리를 추출하여 get_img 함수로 크로핑되고 같은 크기로 바뀐 이미지를 \n",
    "        for index, filename in enumerate(filenames):\n",
    "            bounding_box = bounding_boxes[filename]\n",
    "\n",
    "            try:\n",
    "                # Load images\n",
    "                img_name = '{0}/images/{1}'.format(cub_dataset_dir, filename)\n",
    "                img = get_img(img_name, bounding_box, image_size)\n",
    "                '''\n",
    "                all_embeddings1 = all_embeddings[index, :, :]\n",
    "\n",
    "                embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
    "                '''\n",
    "                embedding = all_embeddings[index, :]\n",
    "                # X = 정제한 이미지 리스트\n",
    "                X.append(np.array(img))\n",
    "                '''\n",
    "                # y = 정제한 이미지 인덱스\n",
    "                y.append(class_ids[index])\n",
    "                '''\n",
    "                # embeddings = \n",
    "                embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        embeddings = np.array(embeddings)\n",
    "        return X, embeddings\n",
    "\n",
    "\n",
    "    def generate_c(x):\n",
    "        mean = x[:, :128]\n",
    "        log_sigma = x[:, 128:]\n",
    "        stddev = K.exp(log_sigma)\n",
    "        epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
    "        c = stddev * epsilon + mean\n",
    "        return c\n",
    "\n",
    "\n",
    "    def build_ca_model():\n",
    "        \"\"\"\n",
    "        (1024,)의 텍스트 인코더 신경망의 출력을 입력으로 받고 (256,) 의 텐서를 출력\n",
    "        \"\"\"\n",
    "        input_layer = Input(shape=(1024,))\n",
    "        x = Dense(256)(input_layer)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        model = Model(inputs=[input_layer], outputs=[x])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def build_embedding_compressor_model():\n",
    "        \"\"\"\n",
    "        입력 속성값 (40,) 을 (128,) 의 벡터로 확장하는 네트워크\n",
    "        \"\"\"\n",
    "        input_layer = Input(shape=(40,))\n",
    "        x = Dense(128)(input_layer)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        model = Model(inputs=[input_layer], outputs=[x])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def build_stage1_generator():\n",
    "        \"\"\"\n",
    "        Stage-I 의 generator \n",
    "        *** 이 신경망 안에 CA 신경망과 생성기 신경망이 들어가 있다!!!! ***\n",
    "        그러므로, 입력으로 텍스트 임베딩 출력 (1024,)과 잡음 변수(100,) 을 받는다\n",
    "        \"\"\"\n",
    "        '''\n",
    "        input_layer = Input(shape=(1024,))\n",
    "        x = Dense(256)(input_layer)\n",
    "        mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        c = Lambda(generate_c)(mean_logsigma)\n",
    "\n",
    "        input_layer2 = Input(shape=(100,))\n",
    "        gen_input = Concatenate(axis=1)([c, input_layer2])\n",
    "        '''\n",
    "        # 텍스트 조건부 변수를 잡음 변수와 접합(concatenation) -> cGAN\n",
    "\n",
    "        input_layer = Input(shape=(40,))\n",
    "        x = Dense(128 * 8 * 4 * 4, use_bias=False)(input_layer)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
    "\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "        x = Activation(activation='tanh')(x)\n",
    "\n",
    "        stage1_gen = Model(inputs=input_layer, outputs=x)\n",
    "        '''\n",
    "        stage - I gen 은 입력된 문장의 임베딩을 바탕으로 (+잡음 변수) 이미지를 생성 함 \n",
    "        '''\n",
    "        return stage1_gen\n",
    "\n",
    "\n",
    "    def build_stage1_discriminator():\n",
    "        \"\"\"\n",
    "        Create a model which takes two inputs\n",
    "        1. One from above network\n",
    "        2. One from the embedding layer\n",
    "        3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
    "        \"\"\"\n",
    "        input_layer = Input(shape=(64, 64, 3))\n",
    "\n",
    "        x = Conv2D(64, (4, 4),\n",
    "                   padding='same', strides=2,\n",
    "                   input_shape=(64, 64, 3), use_bias=False)(input_layer)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        '''\n",
    "        실제 이미지에 해당하는 압축된 임베딩을 입력\n",
    "        '''\n",
    "        input_layer2 = Input(shape=(4, 4, 128))\n",
    "\n",
    "        '''\n",
    "        입력 이미지와 압축 텍스트 임베딩을 합침\n",
    "        '''\n",
    "        merged_input = concatenate([x, input_layer2])\n",
    "\n",
    "        x2 = Conv2D(64 * 8, kernel_size=1,\n",
    "                    padding=\"same\", strides=1)(merged_input)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "        x2 = Flatten()(x2)\n",
    "        x2 = Dense(1)(x2)\n",
    "        x2 = Activation('sigmoid')(x2)\n",
    "\n",
    "        stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=x2)\n",
    "        '''\n",
    "        출력은 입력 이미지가 진짜인지 가짜인지에 관한 확률(sigmoid)을 출력\n",
    "        '''\n",
    "        return stage1_dis\n",
    "\n",
    "\n",
    "    def build_adversarial_model(gen_model, dis_model):\n",
    "        input_layer = Input(shape=(40,))\n",
    "        input_layer3 = Input(shape=(4, 4, 128))\n",
    "\n",
    "        x = gen_model(input_layer)\n",
    "\n",
    "        dis_model.trainable = False\n",
    "        valid = dis_model([x, input_layer3])\n",
    "\n",
    "        model = Model(inputs=[input_layer, input_layer3], outputs=valid)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def KL_loss(y_true, y_pred):\n",
    "        mean = y_pred[:, :128]\n",
    "        logsigma = y_pred[:, :128]\n",
    "        loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
    "        loss = K.mean(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def custom_generator_loss(y_true, y_pred):\n",
    "        # Calculate binary cross entropy loss\n",
    "        return K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "    def save_rgb_img(img, path):\n",
    "        \"\"\"\n",
    "        Save an rgb image\n",
    "        \"\"\"\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"Image\")\n",
    "\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def write_log(callback, name, loss, batch_no):\n",
    "        \"\"\"\n",
    "        Write training summary to TensorBoard\n",
    "        \"\"\"\n",
    "        with callback.as_default():\n",
    "              tf.summary.scalar(name, loss, batch_no)\n",
    "              callback.flush()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    Stage - I stackGAN 훈련\n",
    "    '''\n",
    "    if __name__ == '__main__':\n",
    "        '''\n",
    "        하이퍼파라미터(불변 파라미터) 지정\n",
    "        '''\n",
    "        data_dir = \"C:/Users/user/Desktop/CelebA_dataset_reduce\"\n",
    "        train_dir = data_dir + \"/train\"\n",
    "        test_dir = data_dir + \"/test\"\n",
    "        image_size = 64\n",
    "        batch_size = 100\n",
    "        z_dim = 100\n",
    "        stage1_generator_lr = 0.00008\n",
    "        stage1_discriminator_lr = 0.00008\n",
    "        stage1_lr_decay_step = 600\n",
    "        epochs = 200\n",
    "        condition_dim = 128\n",
    "\n",
    "        embeddings_file_path_train = train_dir + \"/attr_(embeddings).pickle\"\n",
    "        embeddings_file_path_test = test_dir + \"/attr_(embeddings).pickle\"\n",
    "\n",
    "        filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
    "        filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
    "\n",
    "        '''\n",
    "        class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
    "        class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
    "        '''\n",
    "\n",
    "        cub_dataset_dir = data_dir + \"/img_align_celeba\"\n",
    "\n",
    "        '''\n",
    "        optimizer 정의\n",
    "        '''\n",
    "        dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "        gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "        \"\"\"\"\n",
    "        dataset 로드하기\n",
    "        \"\"\"\n",
    "        X_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
    "                                                          cub_dataset_dir=cub_dataset_dir,\n",
    "                                                          embeddings_file_path=embeddings_file_path_train,\n",
    "                                                          image_size=(64, 64))\n",
    "\n",
    "        X_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
    "                                                       cub_dataset_dir=cub_dataset_dir,\n",
    "                                                       embeddings_file_path=embeddings_file_path_test,\n",
    "                                                       image_size=(64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Epoch is: 0\n",
      "Number of batches 160\n",
      "Batch:1\n",
      "d_loss_real:0.6665949821472168\n",
      "d_loss_fake:0.935294508934021\n",
      "d_loss_wrong:3.6151583194732666\n",
      "d_loss:1.4709106981754303\n",
      "g_loss:0.6880786418914795\n",
      "Batch:2\n",
      "d_loss_real:0.5363628268241882\n",
      "d_loss_fake:0.8397202491760254\n",
      "d_loss_wrong:0.5971170663833618\n",
      "d_loss:0.6273907423019409\n",
      "g_loss:0.6994462609291077\n",
      "Batch:3\n",
      "d_loss_real:2.018359899520874\n",
      "d_loss_fake:0.2962876260280609\n",
      "d_loss_wrong:0.6224525570869446\n",
      "d_loss:1.2388649955391884\n",
      "g_loss:0.698293924331665\n",
      "Batch:4\n",
      "d_loss_real:1.6767516136169434\n",
      "d_loss_fake:0.06334748864173889\n",
      "d_loss_wrong:0.9079265594482422\n",
      "d_loss:1.081194318830967\n",
      "g_loss:0.6800373792648315\n",
      "Batch:5\n",
      "d_loss_real:1.2923803329467773\n",
      "d_loss_fake:0.07992007583379745\n",
      "d_loss_wrong:1.1444871425628662\n",
      "d_loss:0.9522919710725546\n",
      "g_loss:0.6675110459327698\n",
      "Batch:6\n",
      "d_loss_real:1.3017162084579468\n",
      "d_loss_fake:0.028034770861268044\n",
      "d_loss_wrong:0.8531692028045654\n",
      "d_loss:0.8711590976454318\n",
      "g_loss:0.652178168296814\n",
      "Batch:7\n",
      "d_loss_real:1.4268532991409302\n",
      "d_loss_fake:0.023288991302251816\n",
      "d_loss_wrong:1.0684788227081299\n",
      "d_loss:0.9863686030730605\n",
      "g_loss:0.6381851434707642\n",
      "Batch:8\n",
      "d_loss_real:1.197687029838562\n",
      "d_loss_fake:0.024893635883927345\n",
      "d_loss_wrong:1.0267609357833862\n",
      "d_loss:0.8617571578361094\n",
      "g_loss:0.6269506216049194\n",
      "Batch:9\n",
      "d_loss_real:1.2901201248168945\n",
      "d_loss_fake:0.011342299170792103\n",
      "d_loss_wrong:1.0235435962677002\n",
      "d_loss:0.9037815362680703\n",
      "g_loss:0.6087092161178589\n",
      "Batch:10\n",
      "d_loss_real:1.3491761684417725\n",
      "d_loss_fake:0.009733373299241066\n",
      "d_loss_wrong:0.9945833086967468\n",
      "d_loss:0.9256672547198832\n",
      "g_loss:0.5919858813285828\n",
      "Batch:11\n",
      "d_loss_real:1.2289767265319824\n",
      "d_loss_fake:0.010030076839029789\n",
      "d_loss_wrong:0.9616944193840027\n",
      "d_loss:0.8574194873217493\n",
      "g_loss:0.5779002904891968\n",
      "Batch:12\n",
      "d_loss_real:1.199788212776184\n",
      "d_loss_fake:0.007885627448558807\n",
      "d_loss_wrong:0.9670451283454895\n",
      "d_loss:0.8436267953366041\n",
      "g_loss:0.559161365032196\n",
      "Batch:13\n",
      "d_loss_real:1.17987859249115\n",
      "d_loss_fake:0.006276512518525124\n",
      "d_loss_wrong:0.9235607981681824\n",
      "d_loss:0.8223986239172518\n",
      "g_loss:0.5433396100997925\n",
      "Batch:14\n",
      "d_loss_real:1.246203064918518\n",
      "d_loss_fake:0.006648542825132608\n",
      "d_loss_wrong:0.9740085005760193\n",
      "d_loss:0.868265793309547\n",
      "g_loss:0.5204960703849792\n",
      "Batch:15\n",
      "d_loss_real:1.2079912424087524\n",
      "d_loss_fake:0.00510570639744401\n",
      "d_loss_wrong:0.9088716506958008\n",
      "d_loss:0.8324899604776874\n",
      "g_loss:0.5117943286895752\n",
      "Batch:16\n",
      "d_loss_real:1.171915888786316\n",
      "d_loss_fake:0.004683881066739559\n",
      "d_loss_wrong:0.9366118907928467\n",
      "d_loss:0.8212818873580545\n",
      "g_loss:0.4849536120891571\n",
      "Batch:17\n",
      "d_loss_real:1.0842396020889282\n",
      "d_loss_fake:0.00472963647916913\n",
      "d_loss_wrong:0.9100634455680847\n",
      "d_loss:0.7708180715562776\n",
      "g_loss:0.4624275863170624\n",
      "Batch:18\n",
      "d_loss_real:1.0982168912887573\n",
      "d_loss_fake:0.003786535235121846\n",
      "d_loss_wrong:0.9617049694061279\n",
      "d_loss:0.7904813218046911\n",
      "g_loss:0.4553483724594116\n",
      "Batch:19\n",
      "d_loss_real:1.1583566665649414\n",
      "d_loss_fake:0.003281806595623493\n",
      "d_loss_wrong:0.9121153354644775\n",
      "d_loss:0.808027618797496\n",
      "g_loss:0.42916321754455566\n",
      "Batch:20\n",
      "d_loss_real:1.1254703998565674\n",
      "d_loss_fake:0.003787311725318432\n",
      "d_loss_wrong:0.891355037689209\n",
      "d_loss:0.7865207872819155\n",
      "g_loss:0.42135730385780334\n",
      "Batch:21\n",
      "d_loss_real:1.1340006589889526\n",
      "d_loss_fake:0.0026313301641494036\n",
      "d_loss_wrong:0.8558635711669922\n",
      "d_loss:0.7816240548272617\n",
      "g_loss:0.41244709491729736\n",
      "Batch:22\n",
      "d_loss_real:1.1051578521728516\n",
      "d_loss_fake:0.002756154164671898\n",
      "d_loss_wrong:0.8639349341392517\n",
      "d_loss:0.7692516981624067\n",
      "g_loss:0.3899686336517334\n",
      "Batch:23\n",
      "d_loss_real:1.1435229778289795\n",
      "d_loss_fake:0.0027288252022117376\n",
      "d_loss_wrong:0.8438302278518677\n",
      "d_loss:0.7834012521780096\n",
      "g_loss:0.3842122554779053\n",
      "Batch:24\n",
      "d_loss_real:1.0603636503219604\n",
      "d_loss_fake:0.003018259070813656\n",
      "d_loss_wrong:0.8957051634788513\n",
      "d_loss:0.7548626807983965\n",
      "g_loss:0.3697797358036041\n",
      "Batch:25\n",
      "d_loss_real:1.0116404294967651\n",
      "d_loss_fake:0.0028944658115506172\n",
      "d_loss_wrong:0.854329526424408\n",
      "d_loss:0.7201262128073722\n",
      "g_loss:0.35771310329437256\n",
      "Batch:26\n",
      "d_loss_real:1.1015948057174683\n",
      "d_loss_fake:0.0024389137979596853\n",
      "d_loss_wrong:0.9021950364112854\n",
      "d_loss:0.7769558904110454\n",
      "g_loss:0.3508860766887665\n",
      "Batch:27\n",
      "d_loss_real:1.0392180681228638\n",
      "d_loss_fake:0.002258096821606159\n",
      "d_loss_wrong:0.8788684010505676\n",
      "d_loss:0.7398906585294753\n",
      "g_loss:0.34497320652008057\n",
      "Batch:28\n",
      "d_loss_real:1.0860638618469238\n",
      "d_loss_fake:0.0023361111525446177\n",
      "d_loss_wrong:0.780485212802887\n",
      "d_loss:0.7387372619123198\n",
      "g_loss:0.33819323778152466\n",
      "Batch:29\n",
      "d_loss_real:1.070005178451538\n",
      "d_loss_fake:0.002244967734441161\n",
      "d_loss_wrong:0.8824288845062256\n",
      "d_loss:0.7561710522859357\n",
      "g_loss:0.33585914969444275\n",
      "Batch:30\n",
      "d_loss_real:1.084407925605774\n",
      "d_loss_fake:0.001998097635805607\n",
      "d_loss_wrong:0.8347063064575195\n",
      "d_loss:0.7513800638262182\n",
      "g_loss:0.3321497440338135\n",
      "Batch:31\n",
      "d_loss_real:1.05708646774292\n",
      "d_loss_fake:0.0017903797561302781\n",
      "d_loss_wrong:0.8511460423469543\n",
      "d_loss:0.7417773393972311\n",
      "g_loss:0.33132049441337585\n",
      "Batch:32\n",
      "d_loss_real:0.9750031232833862\n",
      "d_loss_fake:0.0015694272005930543\n",
      "d_loss_wrong:0.8646138310432434\n",
      "d_loss:0.7040473762026522\n",
      "g_loss:0.3302447497844696\n",
      "Batch:33\n",
      "d_loss_real:1.0464503765106201\n",
      "d_loss_fake:0.001659250701777637\n",
      "d_loss_wrong:0.8505439162254333\n",
      "d_loss:0.7362759799871128\n",
      "g_loss:0.3280690014362335\n",
      "Batch:34\n",
      "d_loss_real:1.0182737112045288\n",
      "d_loss_fake:0.0015944051556289196\n",
      "d_loss_wrong:0.8891162872314453\n",
      "d_loss:0.731814528699033\n",
      "g_loss:0.3277062177658081\n",
      "Batch:35\n",
      "d_loss_real:1.0259429216384888\n",
      "d_loss_fake:0.0015230145072564483\n",
      "d_loss_wrong:0.7841538786888123\n",
      "d_loss:0.7093906841182616\n",
      "g_loss:0.32699286937713623\n",
      "Batch:36\n",
      "d_loss_real:1.0172160863876343\n",
      "d_loss_fake:0.0014187615597620606\n",
      "d_loss_wrong:0.8141658902168274\n",
      "d_loss:0.7125042061379645\n",
      "g_loss:0.32656121253967285\n",
      "Batch:37\n",
      "d_loss_real:1.0622237920761108\n",
      "d_loss_fake:0.0018202110659331083\n",
      "d_loss_wrong:0.7813223600387573\n",
      "d_loss:0.726897538814228\n",
      "g_loss:0.3260553479194641\n",
      "Batch:38\n",
      "d_loss_real:1.0113223791122437\n",
      "d_loss_fake:0.0017385337268933654\n",
      "d_loss_wrong:0.8351963758468628\n",
      "d_loss:0.7148949169495609\n",
      "g_loss:0.3268846273422241\n",
      "Batch:39\n",
      "d_loss_real:1.004647970199585\n",
      "d_loss_fake:0.0016947430558502674\n",
      "d_loss_wrong:0.7914118766784668\n",
      "d_loss:0.7006006400333717\n",
      "g_loss:0.3266250193119049\n",
      "Batch:40\n",
      "d_loss_real:0.9977670907974243\n",
      "d_loss_fake:0.0017009261064231396\n",
      "d_loss_wrong:0.7913704514503479\n",
      "d_loss:0.6971513897879049\n",
      "g_loss:0.3256562054157257\n",
      "Batch:41\n",
      "d_loss_real:0.9815779328346252\n",
      "d_loss_fake:0.0017575508682057261\n",
      "d_loss_wrong:0.8091832995414734\n",
      "d_loss:0.6935241790197324\n",
      "g_loss:0.32641667127609253\n",
      "Batch:42\n",
      "d_loss_real:0.9393529295921326\n",
      "d_loss_fake:0.001439612708054483\n",
      "d_loss_wrong:0.8132266402244568\n",
      "d_loss:0.6733430280291941\n",
      "g_loss:0.3263119161128998\n",
      "Batch:43\n",
      "d_loss_real:1.0367108583450317\n",
      "d_loss_fake:0.0016936329193413258\n",
      "d_loss_wrong:0.7453952431678772\n",
      "d_loss:0.7051276481943205\n",
      "g_loss:0.3253896236419678\n",
      "Batch:44\n",
      "d_loss_real:0.9901605248451233\n",
      "d_loss_fake:0.0016340218717232347\n",
      "d_loss_wrong:0.8150606751441956\n",
      "d_loss:0.6992539366765413\n",
      "g_loss:0.32585692405700684\n",
      "Batch:45\n",
      "d_loss_real:0.997566282749176\n",
      "d_loss_fake:0.0016004173085093498\n",
      "d_loss_wrong:0.8012335896492004\n",
      "d_loss:0.6994916431140155\n",
      "g_loss:0.326798677444458\n",
      "Batch:46\n",
      "d_loss_real:0.9542901515960693\n",
      "d_loss_fake:0.0018592916894704103\n",
      "d_loss_wrong:0.7914227843284607\n",
      "d_loss:0.6754655948025174\n",
      "g_loss:0.32601070404052734\n",
      "Batch:47\n",
      "d_loss_real:0.9856292009353638\n",
      "d_loss_fake:0.0019210113678127527\n",
      "d_loss_wrong:0.7687028646469116\n",
      "d_loss:0.685470569471363\n",
      "g_loss:0.32653355598449707\n",
      "Batch:48\n",
      "d_loss_real:0.999502956867218\n",
      "d_loss_fake:0.0018541663885116577\n",
      "d_loss_wrong:0.7993972301483154\n",
      "d_loss:0.7000643275678158\n",
      "g_loss:0.3254079520702362\n",
      "Batch:49\n",
      "d_loss_real:0.9754496216773987\n",
      "d_loss_fake:0.002061840146780014\n",
      "d_loss_wrong:0.7686340808868408\n",
      "d_loss:0.6803987910971045\n",
      "g_loss:0.3253469169139862\n",
      "Batch:50\n",
      "d_loss_real:0.9730127453804016\n",
      "d_loss_fake:0.0016416602302342653\n",
      "d_loss_wrong:0.7775342464447021\n",
      "d_loss:0.6813003493589349\n",
      "g_loss:0.32536789774894714\n",
      "Batch:51\n",
      "d_loss_real:0.95709627866745\n",
      "d_loss_fake:0.0018995670834556222\n",
      "d_loss_wrong:0.7562450170516968\n",
      "d_loss:0.6680842853675131\n",
      "g_loss:0.32520973682403564\n",
      "Batch:52\n",
      "d_loss_real:0.9853150844573975\n",
      "d_loss_fake:0.002079041674733162\n",
      "d_loss_wrong:0.7679071426391602\n",
      "d_loss:0.6851540883071721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_loss:0.3277546763420105\n",
      "Batch:53\n",
      "d_loss_real:0.9669510722160339\n",
      "d_loss_fake:0.001981888897716999\n",
      "d_loss_wrong:0.7894269227981567\n",
      "d_loss:0.6813277390319854\n",
      "g_loss:0.32562118768692017\n",
      "Batch:54\n",
      "d_loss_real:0.9563271403312683\n",
      "d_loss_fake:0.001962653361260891\n",
      "d_loss_wrong:0.8105707168579102\n",
      "d_loss:0.6812969127204269\n",
      "g_loss:0.3263677954673767\n",
      "Batch:55\n",
      "d_loss_real:0.9723408222198486\n",
      "d_loss_fake:0.0018958220025524497\n",
      "d_loss_wrong:0.7278218269348145\n",
      "d_loss:0.668599823344266\n",
      "g_loss:0.3291727304458618\n",
      "Batch:56\n",
      "d_loss_real:0.9825093150138855\n",
      "d_loss_fake:0.002825761679559946\n",
      "d_loss_wrong:0.7822213172912598\n",
      "d_loss:0.6875164272496477\n",
      "g_loss:0.33002331852912903\n",
      "Batch:57\n",
      "d_loss_real:0.9465724229812622\n",
      "d_loss_fake:0.0014428416034206748\n",
      "d_loss_wrong:0.7703698873519897\n",
      "d_loss:0.6662393937294837\n",
      "g_loss:0.3292708992958069\n",
      "Batch:58\n",
      "d_loss_real:0.9678156971931458\n",
      "d_loss_fake:0.002066501183435321\n",
      "d_loss_wrong:0.7547352910041809\n",
      "d_loss:0.6731082966434769\n",
      "g_loss:0.3256097435951233\n",
      "Batch:59\n",
      "d_loss_real:0.9421607255935669\n",
      "d_loss_fake:0.0021615696605294943\n",
      "d_loss_wrong:0.7353041172027588\n",
      "d_loss:0.6554467845126055\n",
      "g_loss:0.32738617062568665\n",
      "Batch:60\n",
      "d_loss_real:0.9479671716690063\n",
      "d_loss_fake:0.002326360670849681\n",
      "d_loss_wrong:0.7806840538978577\n",
      "d_loss:0.66973618947668\n",
      "g_loss:0.32579872012138367\n",
      "Batch:61\n",
      "d_loss_real:0.9508963823318481\n",
      "d_loss_fake:0.0021354726050049067\n",
      "d_loss_wrong:0.7646121978759766\n",
      "d_loss:0.6671351087861694\n",
      "g_loss:0.3257918655872345\n",
      "Batch:62\n",
      "d_loss_real:0.9507081508636475\n",
      "d_loss_fake:0.0023310210090130568\n",
      "d_loss_wrong:0.7201972603797913\n",
      "d_loss:0.6559861457790248\n",
      "g_loss:0.32559263706207275\n",
      "Batch:63\n",
      "d_loss_real:0.9160510301589966\n",
      "d_loss_fake:0.00205008452758193\n",
      "d_loss_wrong:0.7883936762809753\n",
      "d_loss:0.6556364552816376\n",
      "g_loss:0.3252800703048706\n",
      "Batch:64\n",
      "d_loss_real:0.9470917582511902\n",
      "d_loss_fake:0.0021389927715063095\n",
      "d_loss_wrong:0.7561347484588623\n",
      "d_loss:0.6631143144331872\n",
      "g_loss:0.3253832161426544\n",
      "Batch:65\n",
      "d_loss_real:0.9457835555076599\n",
      "d_loss_fake:0.001796167460270226\n",
      "d_loss_wrong:0.7438256740570068\n",
      "d_loss:0.6592972381331492\n",
      "g_loss:0.32527872920036316\n",
      "Batch:66\n",
      "d_loss_real:0.9506731629371643\n",
      "d_loss_fake:0.0017010628944262862\n",
      "d_loss_wrong:0.7389494776725769\n",
      "d_loss:0.660499216610333\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "        \"\"\"\n",
    "        신경망 빌드 & compile\n",
    "        \"\"\"\n",
    "        '''\n",
    "        ca_model = build_ca_model()\n",
    "        ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "        '''\n",
    "        stage1_dis = build_stage1_discriminator()\n",
    "        stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "        stage1_gen = build_stage1_generator()\n",
    "        stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
    "\n",
    "        embedding_compressor_model = build_embedding_compressor_model()\n",
    "        embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "        '''\n",
    "        stage-I GAN 빌드& 컴파일\n",
    "        이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "        '''\n",
    "        adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
    "        adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
    "                                  optimizer=gen_optimizer, metrics=None)\n",
    "\n",
    "        \"\"\"\n",
    "        tensorboard.set_model(stage1_gen)\n",
    "        tensorboard.set_model(stage1_dis)\n",
    "        '''\n",
    "        tensorboard.set_model(ca_model)\n",
    "        '''\n",
    "        tensorboard.set_model(embedding_compressor_model)\n",
    "        \"\"\"\n",
    "        # Generate an array containing real and fake values\n",
    "        # Apply label smoothing as well\n",
    "        real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
    "        fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
    "        '''\n",
    "        매 epoch 마다 아래를 반복함\n",
    "        '''\n",
    "        for epoch in range(epochs):\n",
    "            print(\"========================================\")\n",
    "            print(\"Epoch is:\", epoch)\n",
    "            print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
    "\n",
    "            gen_losses = []\n",
    "            dis_losses = []\n",
    "\n",
    "            # Load data and train model\n",
    "            number_of_batches = int(X_train.shape[0] / batch_size)\n",
    "            for index in range(number_of_batches):\n",
    "                print(\"Batch:{}\".format(index+1))\n",
    "                '''\n",
    "                모델에 입력으로 들어갈 이미지와 텍스트 임베딩을 받아옴 (각 텍스트 임베딩은 각 이미지에 대응 됨)\n",
    "                '''\n",
    "                # 원래는 CA 의 출력이 stage-I 으로 들어가야 하지만 gen 안에 CA 가 있음\n",
    "                z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "                # 배치 사이즈만큼 훈련(실제) 이미지를 추출\n",
    "                image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # 추출한 이미지에 대응하는 임베딩을 추출\n",
    "                embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # 이미지들을 정규화하여 값을 작게 만듬\n",
    "                image_batch = (image_batch - 127.5) / 127.5\n",
    "\n",
    "                # stage-I 의 gen 에서 텍스트 임베딩을 바탕으로 저 해상도 fake 이미지를 생성\n",
    "                # 이때, 두 stage 는 랜덤하게 생성한 텍스트 임베딩을 나머지 입력으로 받음\n",
    "                fake_images = stage1_gen.predict(embedding_batch, verbose=3)\n",
    "\n",
    "                # stage-I dis 에 들어갈 압축 텍스트 임베딩을 랜덤하게 생성한 텍스트 임베딩 기반으로 생성\n",
    "                compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
    "                compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
    "                compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
    "                '''\n",
    "                discriminator 에서 입력 이미지가 CNN 을 통과한 결과와 속성값 (40,) 을 concatenate 해 주기 위해서는,\n",
    "                속성값 (40,) 를 compressor 네트워크를 통해 (128,) 로 확장하고,\n",
    "                reshape, tile 함수를 이용해 CNN 결과 (4, 4, 512) 와 (4, 4, :) 부분을 맞추어 주어야 한다.\n",
    "                '''\n",
    "                \n",
    "                \"\"\"\n",
    "                stage-I dis 를 훈련함\n",
    "                \"\"\"\n",
    "                # 실제 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 1 (이미지가 진짜라는 의미) 로 하여,\n",
    "                # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "                dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n",
    "                                                          np.reshape(real_labels, (batch_size, 1)))\n",
    "                # gen 이 생성한 가짜 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "                # dis 가 가짜 이미지를 잘 분류 하게끔 훈련\n",
    "                dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n",
    "                                                          np.reshape(fake_labels, (batch_size, 1)))\n",
    "                # 실제 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "                # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "                dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
    "                                                           np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
    "\n",
    "                d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
    "\n",
    "                print(\"d_loss_real:{}\".format(dis_loss_real))\n",
    "                print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
    "                print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
    "                print(\"d_loss:{}\".format(d_loss))\n",
    "\n",
    "                \"\"\"\n",
    "                stage-I GAN 을 훈련함\n",
    "                이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "                \"\"\"\n",
    "                g_loss = adversarial_model.train_on_batch([embedding_batch, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
    "                print(\"g_loss:{}\".format(g_loss))\n",
    "\n",
    "                dis_losses.append(d_loss)\n",
    "                gen_losses.append(g_loss)\n",
    "\n",
    "            \"\"\"\n",
    "            각 epoch 마다 Tensorboard 에 loss 저장\n",
    "            \"\"\"\n",
    "            writer = tf.summary.create_file_writer(\"logs/\".format(time.time()))\n",
    "            write_log(writer, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
    "            write_log(writer, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "            '''\n",
    "            tf.summary.scalar('discriminator_loss', np.mean(dis_losses), epoch)\n",
    "            tf.summary.scalar('generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "            \n",
    "            tf.summary.scalar('discriminator_loss', np.mean(dis_losses))\n",
    "            tf.summary.scalar('generator_loss', np.mean(gen_losses[0]))\n",
    "            summary_op = tf.summary.merge()\n",
    "            summary_writer = tf.summary.FileWriter(\"logs/\".format(time.time()))\n",
    "            '''    \n",
    "            # 매 두번의 epoch 마다 이미지 gen & 이미지 저장\n",
    "            if epoch % 2 == 0:\n",
    "                # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "                z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "                embedding_batch = embeddings_test[0:batch_size]\n",
    "                fake_images = stage1_gen.predict_on_batch(embedding_batch)\n",
    "\n",
    "                # Save images\n",
    "                for i, img in enumerate(fake_images[:10]):\n",
    "                    save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))\n",
    "                    \n",
    "            if epoch % 50 == 0:\n",
    "                # Save models\n",
    "                stage1_gen.save_weights(\"weighs/stage1_gen.h5_epoch_{0}\".format(epochs))\n",
    "                stage1_dis.save_weights(\"weighs/stage1_dis.h5_epoch_{0}\".format(epochs))\n",
    "\n",
    "        '''\n",
    "        이제 훈련된 stage-I 의 generator 와 discriminator 을 얻음 (+ embedding_compressor) \n",
    "        '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
