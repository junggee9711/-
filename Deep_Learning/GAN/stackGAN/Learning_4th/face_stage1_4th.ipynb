{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uniform-syndication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings:  (16000, 40)\n",
      "Embeddings shape: (16000, 40)\n",
      "embeddings:  (4000, 40)\n",
      "Embeddings shape: (4000, 40)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
    "    concatenate, Flatten, Lambda, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "   \n",
    "def load_class_ids(class_info_file_path):\n",
    "    \"\"\"\n",
    "    Load class ids from class_info.pickle file\n",
    "    \"\"\"\n",
    "    with open(class_info_file_path, 'rb') as f:\n",
    "        class_ids = pickle.load(f, encoding='latin1')\n",
    "        return class_ids\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_file_path):\n",
    "    \"\"\"\n",
    "    훈련된 텍스트 임베딩을 불러옴\n",
    "    \"\"\"\n",
    "    with open(embeddings_file_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f, encoding='latin1')\n",
    "        embeddings = np.array(embeddings)\n",
    "        print('embeddings: ', embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "def load_filenames(filenames_file_path):\n",
    "    \"\"\"\n",
    "    Load filenames.pickle file and return a list of all file names\n",
    "    \"\"\"\n",
    "    with open(filenames_file_path, 'rb') as f:\n",
    "        filenames = pickle.load(f, encoding='latin1')\n",
    "    return filenames\n",
    "\n",
    "def load_bounding_boxes(dataset_dir):\n",
    "    \"\"\"\n",
    "    이미지와 그에 상응하는 바운딩 박스를 짝지어 딕셔너리로 만들어 출력\n",
    "    \"\"\"\n",
    "    # 바운딩 박스 전체 경로\n",
    "    bounding_boxes_path = os.path.join(dataset_dir, 'list_bbox_celeba_pure.csv')\n",
    "    file_paths_path = os.path.join(dataset_dir, 'list_filenames.csv')\n",
    "\n",
    "    # bounding_boxes.txt 와 images.txt 파일을 읽어옴\n",
    "    df_bounding_boxes = pd.read_csv(bounding_boxes_path, header=None).astype(int)\n",
    "    df_file_names = pd.read_csv(file_paths_path, header=None)\n",
    "\n",
    "    # 전체 이미지 파일 명이 순서대로 적힌 리스트를 만듬\n",
    "    file_names = df_file_names[0].tolist() \n",
    "\n",
    "    # 파일 이름에 대응하는 바운딩 박스가 들어갈 딕셔너리를 만듬 (딕셔너리는 크기를 임의로 증가시킬수 있으므로 초기 사이즈는 아무렇게나)\n",
    "    filename_boundingbox_dict = {}\n",
    "\n",
    "    # 이미지 파일과 그에 해당하는 바운딩 박스를 딕셔너리로 만듬 (key = 이미지 파일 이름)\n",
    "    for i in range(0, len(file_names)):\n",
    "        # Get the bounding box\n",
    "        bounding_box = df_bounding_boxes.iloc[i][:].tolist()\n",
    "        key = file_names[i][:-4] + '.jpg'\n",
    "        filename_boundingbox_dict[key] = bounding_box\n",
    "\n",
    "    return filename_boundingbox_dict\n",
    "'''\n",
    "새 이미지가 크롭핑 되어있지 않기 크롭하기 위한 바운딩 박스 좌표 값이 파일에 주어지며,\n",
    "그 파일을 토대로 이미지를 크로핑 한 후,\n",
    "크로핑된 모든 이미지를 지정한 이미지 크기 (image_size) 값으로 바꾼다\n",
    "'''\n",
    "def get_img(img_path, bbox, image_size):\n",
    "    \"\"\"\n",
    "    Load and resize image\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    width, height = img.size\n",
    "    if bbox is not None:\n",
    "        pass\n",
    "    '''\n",
    "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "        y1 = np.maximum(0, center_y - R)\n",
    "        y2 = np.minimum(height, center_y + R)\n",
    "        x1 = np.maximum(0, center_x - R)\n",
    "        x2 = np.minimum(width, center_x + R)\n",
    "        img = img.crop([x1, y1, x2, y2])\n",
    "    '''\n",
    "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_dataset(filenames_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    filenames = load_filenames(filenames_file_path)\n",
    "    '''\n",
    "    class_ids = load_class_ids(class_info_file_path)\n",
    "    '''\n",
    "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
    "    all_embeddings = load_embeddings(embeddings_file_path)\n",
    "\n",
    "    X, y, embeddings = [], [], []\n",
    "\n",
    "    print(\"Embeddings shape:\", all_embeddings.shape)\n",
    "\n",
    "    # 각 이미지에 해당하는 바운딩 박스 딕셔너리를 추출하여 get_img 함수로 크로핑되고 같은 크기로 바뀐 이미지를 \n",
    "    for index, filename in enumerate(filenames):\n",
    "        bounding_box = bounding_boxes[filename]\n",
    "\n",
    "        try:\n",
    "            # Load images\n",
    "            img_name = '{0}/images/{1}'.format(cub_dataset_dir, filename)\n",
    "            img = get_img(img_name, bounding_box, image_size)\n",
    "            '''\n",
    "            all_embeddings1 = all_embeddings[index, :, :]\n",
    "\n",
    "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
    "            '''\n",
    "            embedding = all_embeddings[index, :]\n",
    "            # X = 정제한 이미지 리스트\n",
    "            X.append(np.array(img))\n",
    "            '''\n",
    "            # y = 정제한 이미지 인덱스\n",
    "            y.append(class_ids[index])\n",
    "            '''\n",
    "            # embeddings = \n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return X, embeddings\n",
    "\n",
    "\n",
    "def generate_c(x):\n",
    "    mean = x[:, :128]\n",
    "    log_sigma = x[:, 128:]\n",
    "    stddev = K.exp(log_sigma)\n",
    "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
    "    c = stddev * epsilon + mean\n",
    "    return c\n",
    "\n",
    "\n",
    "def build_ca_model():\n",
    "    \"\"\"\n",
    "    (1024,)의 텍스트 인코더 신경망의 출력을 입력으로 받고 (256,) 의 텐서를 출력\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(1024,))\n",
    "    x = Dense(256)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    model = Model(inputs=[input_layer], outputs=[x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_embedding_compressor_model():\n",
    "    \"\"\"\n",
    "    입력 속성값 (40,) 을 (128,) 의 벡터로 확장하는 네트워크\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(40,))\n",
    "    x = Dense(10)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=[x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_stage1_generator():\n",
    "    \"\"\"\n",
    "    Stage-I 의 generator \n",
    "    *** 이 신경망 안에 CA 신경망과 생성기 신경망이 들어가 있다!!!! ***\n",
    "    그러므로, 입력으로 텍스트 임베딩 출력 (1024,)과 잡음 변수(100,) 을 받는다\n",
    "    \"\"\"\n",
    "    '''\n",
    "    input_layer = Input(shape=(1024,))\n",
    "    x = Dense(256)(input_layer)\n",
    "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    c = Lambda(generate_c)(mean_logsigma)\n",
    "\n",
    "    input_layer2 = Input(shape=(100,))\n",
    "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
    "    '''\n",
    "    # 텍스트 조건부 변수를 잡음 변수와 접합(concatenation) -> cGAN\n",
    "\n",
    "    input_layer = Input(shape=(40,))\n",
    "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = Activation(activation='tanh')(x)\n",
    "\n",
    "    stage1_gen = Model(inputs=input_layer, outputs=x)\n",
    "    '''\n",
    "    stage - I gen 은 입력된 문장의 임베딩을 바탕으로 (+잡음 변수) 이미지를 생성 함 \n",
    "    '''\n",
    "    return stage1_gen\n",
    "\n",
    "\n",
    "def build_stage1_discriminator():\n",
    "    \"\"\"\n",
    "    Create a model which takes two inputs\n",
    "    1. One from above network\n",
    "    2. One from the embedding layer\n",
    "    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(64, 64, 3))\n",
    "\n",
    "    x = Conv2D(64, (4, 4),\n",
    "               padding='same', strides=2,\n",
    "               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    '''\n",
    "    실제 이미지에 해당하는 압축된 임베딩을 입력\n",
    "    '''\n",
    "    input_layer2 = Input(shape=(4, 4, 10))\n",
    "\n",
    "    '''\n",
    "    입력 이미지와 압축 텍스트 임베딩을 합침\n",
    "    '''\n",
    "    merged_input = concatenate([x, input_layer2])\n",
    "\n",
    "    x2 = Conv2D(64 * 8, kernel_size=1,\n",
    "                padding=\"same\", strides=1)(merged_input)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    x2 = Dense(1)(x2)\n",
    "    x2 = Activation('sigmoid')(x2)\n",
    "\n",
    "    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=x2)\n",
    "    '''\n",
    "    출력은 입력 이미지가 진짜인지 가짜인지에 관한 확률(sigmoid)을 출력\n",
    "    '''\n",
    "    return stage1_dis\n",
    "\n",
    "\n",
    "def build_adversarial_model(gen_model, dis_model):\n",
    "    input_layer = Input(shape=(40,))\n",
    "    input_layer3 = Input(shape=(4, 4, 10))\n",
    "\n",
    "    x = gen_model(input_layer)\n",
    "\n",
    "    dis_model.trainable = False\n",
    "    valid = dis_model([x, input_layer3])\n",
    "\n",
    "    model = Model(inputs=[input_layer, input_layer3], outputs=valid)\n",
    "    return model\n",
    "\n",
    "\n",
    "def KL_loss(y_true, y_pred):\n",
    "    mean = y_pred[:, :128]\n",
    "    logsigma = y_pred[:, :128]\n",
    "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
    "    loss = K.mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def custom_generator_loss(y_true, y_pred):\n",
    "    # Calculate binary cross entropy loss\n",
    "    return K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "def save_rgb_img(img, path):\n",
    "    \"\"\"\n",
    "    Save an rgb image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Image\")\n",
    "\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def write_log(callback, name, loss, batch_no):\n",
    "    \"\"\"\n",
    "    Write training summary to TensorBoard\n",
    "    \"\"\"\n",
    "    with callback.as_default():\n",
    "          tf.summary.scalar(name, loss, batch_no)\n",
    "          callback.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Stage - I stackGAN 훈련\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    하이퍼파라미터(불변 파라미터) 지정\n",
    "    '''\n",
    "    data_dir = \"/home/csle/Desktop/CelebA_dataset_reduce\"\n",
    "    train_dir = data_dir + \"/train\"\n",
    "    test_dir = data_dir + \"/test\"\n",
    "    image_size = 64\n",
    "    batch_size = 128\n",
    "    z_dim = 100\n",
    "    stage1_generator_lr = 0.0002\n",
    "    stage1_discriminator_lr = 0.0002\n",
    "    stage1_lr_decay_step = 600\n",
    "    epochs = 200\n",
    "    condition_dim = 10\n",
    "\n",
    "    embeddings_file_path_train = train_dir + \"/attr_(embeddings).pickle\"\n",
    "    embeddings_file_path_test = test_dir + \"/attr_(embeddings).pickle\"\n",
    "\n",
    "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
    "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
    "\n",
    "    '''\n",
    "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
    "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
    "    '''\n",
    "\n",
    "    cub_dataset_dir = data_dir + \"/img_align_celeba\"\n",
    "\n",
    "    '''\n",
    "    optimizer 정의\n",
    "    '''\n",
    "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    \"\"\"\"\n",
    "    dataset 로드하기\n",
    "    \"\"\"\n",
    "    X_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
    "                                                      cub_dataset_dir=cub_dataset_dir,\n",
    "                                                      embeddings_file_path=embeddings_file_path_train,\n",
    "                                                      image_size=(64, 64))\n",
    "\n",
    "    X_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
    "                                                   cub_dataset_dir=cub_dataset_dir,\n",
    "                                                   embeddings_file_path=embeddings_file_path_test,\n",
    "                                                   image_size=(64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quantitative-smoke",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Epoch is: 0\n",
      "Number of batches 125\n",
      "Batch:1\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f26317413b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2573541b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "d_loss_real:0.5674400329589844\n",
      "d_loss_fake:3.93941330909729\n",
      "d_loss_wrong:8.893132209777832\n",
      "d_loss:3.4918563961982727\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x7f2572dc7050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "g_loss:0.7205632925033569\n",
      "Batch:2\n",
      "d_loss_real:0.4806445837020874\n",
      "d_loss_fake:3.3400704860687256\n",
      "d_loss_wrong:0.07563874870538712\n",
      "d_loss:1.0942496005445719\n",
      "g_loss:0.790033221244812\n",
      "Batch:3\n",
      "d_loss_real:4.4705915451049805\n",
      "d_loss_fake:0.019957419484853745\n",
      "d_loss_wrong:1.3215755224227905\n",
      "d_loss:2.5706790080294013\n",
      "g_loss:0.7175033092498779\n",
      "Batch:4\n",
      "d_loss_real:1.7763992547988892\n",
      "d_loss_fake:0.9410151839256287\n",
      "d_loss_wrong:1.3400355577468872\n",
      "d_loss:1.4584623128175735\n",
      "g_loss:0.8007746338844299\n",
      "Batch:5\n",
      "d_loss_real:2.490717649459839\n",
      "d_loss_fake:0.0016522035002708435\n",
      "d_loss_wrong:1.1598907709121704\n",
      "d_loss:1.5357445683330297\n",
      "g_loss:0.7416332364082336\n",
      "Batch:6\n",
      "d_loss_real:1.8573157787322998\n",
      "d_loss_fake:0.406360924243927\n",
      "d_loss_wrong:1.0868208408355713\n",
      "d_loss:1.3019533306360245\n",
      "g_loss:0.7571575045585632\n",
      "Batch:7\n",
      "d_loss_real:1.9605002403259277\n",
      "d_loss_fake:0.0006195843452587724\n",
      "d_loss_wrong:1.239121913909912\n",
      "d_loss:1.2901854947267566\n",
      "g_loss:0.7208679914474487\n",
      "Batch:8\n",
      "d_loss_real:1.518510341644287\n",
      "d_loss_fake:0.012600746005773544\n",
      "d_loss_wrong:1.2116895914077759\n",
      "d_loss:1.065327755175531\n",
      "g_loss:0.6590180993080139\n",
      "Batch:9\n",
      "d_loss_real:1.571465015411377\n",
      "d_loss_fake:0.435141384601593\n",
      "d_loss_wrong:0.8426530957221985\n",
      "d_loss:1.1051811277866364\n",
      "g_loss:0.7200498580932617\n",
      "Batch:10\n",
      "d_loss_real:1.9699900150299072\n",
      "d_loss_fake:0.0001212868228321895\n",
      "d_loss_wrong:1.027740240097046\n",
      "d_loss:1.2419603892449231\n",
      "g_loss:0.6580581068992615\n",
      "Batch:11\n",
      "d_loss_real:1.3572602272033691\n",
      "d_loss_fake:0.0005846577696502209\n",
      "d_loss_wrong:1.2471556663513184\n",
      "d_loss:0.9905651946319267\n",
      "g_loss:0.6099109649658203\n",
      "Batch:12\n",
      "d_loss_real:1.3853917121887207\n",
      "d_loss_fake:0.005216456949710846\n",
      "d_loss_wrong:1.0573838949203491\n",
      "d_loss:0.9583459440618753\n",
      "g_loss:0.5708462595939636\n",
      "Batch:13\n",
      "d_loss_real:1.4469647407531738\n",
      "d_loss_fake:0.030543465167284012\n",
      "d_loss_wrong:1.0009727478027344\n",
      "d_loss:0.9813614236190915\n",
      "g_loss:0.5369240045547485\n",
      "Batch:14\n",
      "d_loss_real:1.254341959953308\n",
      "d_loss_fake:0.0040946463122963905\n",
      "d_loss_wrong:1.0543044805526733\n",
      "d_loss:0.8917707616928965\n",
      "g_loss:0.5052354335784912\n",
      "Batch:15\n",
      "d_loss_real:1.3299341201782227\n",
      "d_loss_fake:0.004737370647490025\n",
      "d_loss_wrong:1.0455021858215332\n",
      "d_loss:0.9275269492063671\n",
      "g_loss:0.4794348180294037\n",
      "Batch:16\n",
      "d_loss_real:1.3107165098190308\n",
      "d_loss_fake:0.0057373023591935635\n",
      "d_loss_wrong:0.9550747871398926\n",
      "d_loss:0.8955612772842869\n",
      "g_loss:0.45967555046081543\n",
      "Batch:17\n",
      "d_loss_real:1.244922399520874\n",
      "d_loss_fake:0.004095287062227726\n",
      "d_loss_wrong:1.003876805305481\n",
      "d_loss:0.8744542228523642\n",
      "g_loss:0.43609118461608887\n",
      "Batch:18\n",
      "d_loss_real:1.258360743522644\n",
      "d_loss_fake:0.0041203624568879604\n",
      "d_loss_wrong:1.0311294794082642\n",
      "d_loss:0.88799283222761\n",
      "g_loss:0.41870275139808655\n",
      "Batch:19\n",
      "d_loss_real:1.213347315788269\n",
      "d_loss_fake:0.0037103663198649883\n",
      "d_loss_wrong:0.9493406414985657\n",
      "d_loss:0.8449364098487422\n",
      "g_loss:0.3999146819114685\n",
      "Batch:20\n",
      "d_loss_real:1.20365309715271\n",
      "d_loss_fake:0.0034030235838145018\n",
      "d_loss_wrong:0.9153345823287964\n",
      "d_loss:0.8315109500545077\n",
      "g_loss:0.38040703535079956\n",
      "Batch:21\n",
      "d_loss_real:1.1092256307601929\n",
      "d_loss_fake:0.003494713921099901\n",
      "d_loss_wrong:1.001015305519104\n",
      "d_loss:0.8057403202401474\n",
      "g_loss:0.3702477216720581\n",
      "Batch:22\n",
      "d_loss_real:1.2270466089248657\n",
      "d_loss_fake:0.0036238860338926315\n",
      "d_loss_wrong:0.8476840853691101\n",
      "d_loss:0.8263502973131835\n",
      "g_loss:0.35835474729537964\n",
      "Batch:23\n",
      "d_loss_real:1.127729892730713\n",
      "d_loss_fake:0.0029775153379887342\n",
      "d_loss_wrong:0.9587401747703552\n",
      "d_loss:0.8042943688924424\n",
      "g_loss:0.3505828380584717\n",
      "Batch:24\n",
      "d_loss_real:1.111985683441162\n",
      "d_loss_fake:0.0035778023302555084\n",
      "d_loss_wrong:0.8853679895401001\n",
      "d_loss:0.77822928968817\n",
      "g_loss:0.34521597623825073\n",
      "Batch:25\n",
      "d_loss_real:1.0950970649719238\n",
      "d_loss_fake:0.003138488857075572\n",
      "d_loss_wrong:0.9515990018844604\n",
      "d_loss:0.7862329051713459\n",
      "g_loss:0.3396867513656616\n",
      "Batch:26\n",
      "d_loss_real:1.0341922044754028\n",
      "d_loss_fake:0.0021246233955025673\n",
      "d_loss_wrong:0.9108445644378662\n",
      "d_loss:0.7453383991960436\n",
      "g_loss:0.3352147340774536\n",
      "Batch:27\n",
      "d_loss_real:1.165798306465149\n",
      "d_loss_fake:0.002223488176241517\n",
      "d_loss_wrong:0.8366827964782715\n",
      "d_loss:0.7926257243962027\n",
      "g_loss:0.33228379487991333\n",
      "Batch:28\n",
      "d_loss_real:1.0958423614501953\n",
      "d_loss_fake:0.0020302121993154287\n",
      "d_loss_wrong:0.8988809585571289\n",
      "d_loss:0.7731489734142087\n",
      "g_loss:0.3300943374633789\n",
      "Batch:29\n",
      "d_loss_real:1.1008954048156738\n",
      "d_loss_fake:0.0021504308097064495\n",
      "d_loss_wrong:0.8754778504371643\n",
      "d_loss:0.7698547727195546\n",
      "g_loss:0.32760459184646606\n",
      "Batch:30\n",
      "d_loss_real:1.0765304565429688\n",
      "d_loss_fake:0.0017869733273983002\n",
      "d_loss_wrong:0.8746544718742371\n",
      "d_loss:0.7573755895718932\n",
      "g_loss:0.328546941280365\n",
      "Batch:31\n",
      "d_loss_real:1.0601880550384521\n",
      "d_loss_fake:0.001836909563280642\n",
      "d_loss_wrong:0.8572887778282166\n",
      "d_loss:0.7448754493671004\n",
      "g_loss:0.3272940516471863\n",
      "Batch:32\n",
      "d_loss_real:1.0577433109283447\n",
      "d_loss_fake:0.001253196969628334\n",
      "d_loss_wrong:0.8236995339393616\n",
      "d_loss:0.7351098381914198\n",
      "g_loss:0.3267894983291626\n",
      "Batch:33\n",
      "d_loss_real:1.0206890106201172\n",
      "d_loss_fake:0.0014091359917074442\n",
      "d_loss_wrong:0.8710029721260071\n",
      "d_loss:0.7284475323394872\n",
      "g_loss:0.32658252120018005\n",
      "Batch:34\n",
      "d_loss_real:1.100240707397461\n",
      "d_loss_fake:0.001395499100908637\n",
      "d_loss_wrong:0.803294837474823\n",
      "d_loss:0.7512929378426634\n",
      "g_loss:0.3258358836174011\n",
      "Batch:35\n",
      "d_loss_real:1.0660271644592285\n",
      "d_loss_fake:0.0013060050550848246\n",
      "d_loss_wrong:0.8946003317832947\n",
      "d_loss:0.7569901664392091\n",
      "g_loss:0.3259892165660858\n",
      "Batch:36\n",
      "d_loss_real:0.9969518780708313\n",
      "d_loss_fake:0.001329872291535139\n",
      "d_loss_wrong:0.8355430364608765\n",
      "d_loss:0.7076941662235186\n",
      "g_loss:0.32547277212142944\n",
      "Batch:37\n",
      "d_loss_real:1.06187903881073\n",
      "d_loss_fake:0.0012818141840398312\n",
      "d_loss_wrong:0.8745469450950623\n",
      "d_loss:0.7498967092251405\n",
      "g_loss:0.3263716697692871\n",
      "Batch:38\n",
      "d_loss_real:1.0296494960784912\n",
      "d_loss_fake:0.001264000777155161\n",
      "d_loss_wrong:0.8306217789649963\n",
      "d_loss:0.7227961929747835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_loss:0.3256359100341797\n",
      "Batch:39\n",
      "d_loss_real:1.0105854272842407\n",
      "d_loss_fake:0.001315642730332911\n",
      "d_loss_wrong:0.8377246856689453\n",
      "d_loss:0.7150527957419399\n",
      "g_loss:0.32556819915771484\n",
      "Batch:40\n",
      "d_loss_real:1.0535260438919067\n",
      "d_loss_fake:0.0013164648553356528\n",
      "d_loss_wrong:0.7762072682380676\n",
      "d_loss:0.7211439552193042\n",
      "g_loss:0.32534152269363403\n",
      "Batch:41\n",
      "d_loss_real:1.0009241104125977\n",
      "d_loss_fake:0.0014823715901002288\n",
      "d_loss_wrong:0.8479294776916504\n",
      "d_loss:0.7128150175267365\n",
      "g_loss:0.32694923877716064\n",
      "Batch:42\n",
      "d_loss_real:1.0122570991516113\n",
      "d_loss_fake:0.0014736742014065385\n",
      "d_loss_wrong:0.8330556750297546\n",
      "d_loss:0.714760886883596\n",
      "g_loss:0.32527846097946167\n",
      "Batch:43\n",
      "d_loss_real:0.9823076725006104\n",
      "d_loss_fake:0.001914338325150311\n",
      "d_loss_wrong:0.8415480256080627\n",
      "d_loss:0.7020194272336084\n",
      "g_loss:0.3252413868904114\n",
      "Batch:44\n",
      "d_loss_real:1.0798574686050415\n",
      "d_loss_fake:0.0016665365546941757\n",
      "d_loss_wrong:0.7479703426361084\n",
      "d_loss:0.7273379541002214\n",
      "g_loss:0.3253178596496582\n",
      "Batch:45\n",
      "d_loss_real:0.9866930246353149\n",
      "d_loss_fake:0.0016483444487676024\n",
      "d_loss_wrong:0.8511160612106323\n",
      "d_loss:0.7065376137325075\n",
      "g_loss:0.325350821018219\n",
      "Batch:46\n",
      "d_loss_real:1.0203375816345215\n",
      "d_loss_fake:0.0017072651535272598\n",
      "d_loss_wrong:0.7535867691040039\n",
      "d_loss:0.6989922993816435\n",
      "g_loss:0.3263378441333771\n",
      "Batch:47\n",
      "d_loss_real:0.9693889617919922\n",
      "d_loss_fake:0.0016796339768916368\n",
      "d_loss_wrong:0.864029586315155\n",
      "d_loss:0.7011217859690078\n",
      "g_loss:0.32738548517227173\n",
      "Batch:48\n",
      "d_loss_real:0.9922057390213013\n",
      "d_loss_fake:0.0016498842742294073\n",
      "d_loss_wrong:0.7636479735374451\n",
      "d_loss:0.6874273339635693\n",
      "g_loss:0.3308252692222595\n",
      "Batch:49\n",
      "d_loss_real:0.9974362850189209\n",
      "d_loss_fake:0.0016338713467121124\n",
      "d_loss_wrong:0.8212707042694092\n",
      "d_loss:0.7044442864134908\n",
      "g_loss:0.32520025968551636\n",
      "Batch:50\n",
      "d_loss_real:0.983426570892334\n",
      "d_loss_fake:0.0019887899979948997\n",
      "d_loss_wrong:0.8144101500511169\n",
      "d_loss:0.695813020458445\n",
      "g_loss:0.3266714811325073\n",
      "Batch:51\n",
      "d_loss_real:0.9899824261665344\n",
      "d_loss_fake:0.0021397825330495834\n",
      "d_loss_wrong:0.7772017121315002\n",
      "d_loss:0.6898265867494047\n",
      "g_loss:0.3260481357574463\n",
      "Batch:52\n",
      "d_loss_real:1.0139808654785156\n",
      "d_loss_fake:0.002315335441380739\n",
      "d_loss_wrong:0.770961582660675\n",
      "d_loss:0.7003096622647718\n",
      "g_loss:0.32578790187835693\n",
      "Batch:53\n",
      "d_loss_real:0.9684067964553833\n",
      "d_loss_fake:0.0025985976681113243\n",
      "d_loss_wrong:0.8092922568321228\n",
      "d_loss:0.6871761118527502\n",
      "g_loss:0.3257051706314087\n",
      "Batch:54\n",
      "d_loss_real:0.9852550029754639\n",
      "d_loss_fake:0.001644923584535718\n",
      "d_loss_wrong:0.7731806039810181\n",
      "d_loss:0.6863338833791204\n",
      "g_loss:0.32518285512924194\n",
      "Batch:55\n",
      "d_loss_real:0.9657335877418518\n",
      "d_loss_fake:0.0013718714471906424\n",
      "d_loss_wrong:0.7942687273025513\n",
      "d_loss:0.6817769435583614\n",
      "g_loss:0.3252539038658142\n",
      "Batch:56\n",
      "d_loss_real:1.0083129405975342\n",
      "d_loss_fake:0.001351949293166399\n",
      "d_loss_wrong:0.7467334270477295\n",
      "d_loss:0.6911778143839911\n",
      "g_loss:0.32559841871261597\n",
      "Batch:57\n",
      "d_loss_real:0.9602829217910767\n",
      "d_loss_fake:0.0012059470172971487\n",
      "d_loss_wrong:0.8056367635726929\n",
      "d_loss:0.6818521385430358\n",
      "g_loss:0.3251647651195526\n",
      "Batch:58\n",
      "d_loss_real:0.952029824256897\n",
      "d_loss_fake:0.0011395509354770184\n",
      "d_loss_wrong:0.7820080518722534\n",
      "d_loss:0.6718018128303811\n",
      "g_loss:0.32525163888931274\n",
      "Batch:59\n",
      "d_loss_real:0.9711744785308838\n",
      "d_loss_fake:0.001135007943958044\n",
      "d_loss_wrong:0.7623233199119568\n",
      "d_loss:0.6764518212294206\n",
      "g_loss:0.32521241903305054\n",
      "Batch:60\n",
      "d_loss_real:0.953405499458313\n",
      "d_loss_fake:0.0009594400180503726\n",
      "d_loss_wrong:0.7504109740257263\n",
      "d_loss:0.6645453532401007\n",
      "g_loss:0.32517924904823303\n",
      "Batch:61\n",
      "d_loss_real:0.9898031949996948\n",
      "d_loss_fake:0.0010128740686923265\n",
      "d_loss_wrong:0.7939556241035461\n",
      "d_loss:0.693643722042907\n",
      "g_loss:0.3251958191394806\n",
      "Batch:62\n",
      "d_loss_real:0.9510867595672607\n",
      "d_loss_fake:0.0011204101610928774\n",
      "d_loss_wrong:0.7912002801895142\n",
      "d_loss:0.6736235523712821\n",
      "g_loss:0.3251795172691345\n",
      "Batch:63\n",
      "d_loss_real:0.9465957880020142\n",
      "d_loss_fake:0.0008168690837919712\n",
      "d_loss_wrong:0.7577536702156067\n",
      "d_loss:0.6629405288258567\n",
      "g_loss:0.3251422047615051\n",
      "Batch:64\n",
      "d_loss_real:0.9805182218551636\n",
      "d_loss_fake:0.0010578040964901447\n",
      "d_loss_wrong:0.7500890493392944\n",
      "d_loss:0.6780458242865279\n",
      "g_loss:0.32520991563796997\n",
      "Batch:65\n",
      "d_loss_real:0.9671387672424316\n",
      "d_loss_fake:0.00098832743242383\n",
      "d_loss_wrong:0.7616152763366699\n",
      "d_loss:0.6742202845634893\n",
      "g_loss:0.3251195549964905\n",
      "Batch:66\n",
      "d_loss_real:0.9367355108261108\n",
      "d_loss_fake:0.0008587973425164819\n",
      "d_loss_wrong:0.7699165940284729\n",
      "d_loss:0.6610616032558028\n",
      "g_loss:0.32514867186546326\n",
      "Batch:67\n",
      "d_loss_real:0.9334916472434998\n",
      "d_loss_fake:0.0007059745257720351\n",
      "d_loss_wrong:0.7623021602630615\n",
      "d_loss:0.6574978573189583\n",
      "g_loss:0.32512784004211426\n",
      "Batch:68\n",
      "d_loss_real:0.9667515754699707\n",
      "d_loss_fake:0.000721117714419961\n",
      "d_loss_wrong:0.7509173154830933\n",
      "d_loss:0.6712853960343637\n",
      "g_loss:0.32513514161109924\n",
      "Batch:69\n",
      "d_loss_real:0.9437188506126404\n",
      "d_loss_fake:0.0008938998216763139\n",
      "d_loss_wrong:0.7646868824958801\n",
      "d_loss:0.6632546208857093\n",
      "g_loss:0.3251446485519409\n",
      "Batch:70\n",
      "d_loss_real:0.9550610780715942\n",
      "d_loss_fake:0.000839573098346591\n",
      "d_loss_wrong:0.7627633213996887\n",
      "d_loss:0.668431262660306\n",
      "g_loss:0.3251398801803589\n",
      "Batch:71\n",
      "d_loss_real:0.93822181224823\n",
      "d_loss_fake:0.0008834467153064907\n",
      "d_loss_wrong:0.7461820840835571\n",
      "d_loss:0.6558772888238309\n",
      "g_loss:0.32519349455833435\n",
      "Batch:72\n",
      "d_loss_real:0.9577405452728271\n",
      "d_loss_fake:0.0007465381640940905\n",
      "d_loss_wrong:0.7654132843017578\n",
      "d_loss:0.6704102282528765\n",
      "g_loss:0.3254632353782654\n",
      "Batch:73\n",
      "d_loss_real:0.9249305725097656\n",
      "d_loss_fake:0.0008239432354457676\n",
      "d_loss_wrong:0.7573521137237549\n",
      "d_loss:0.652009300494683\n",
      "g_loss:0.3265179395675659\n",
      "Batch:74\n",
      "d_loss_real:0.9393479228019714\n",
      "d_loss_fake:0.0008845372940413654\n",
      "d_loss_wrong:0.7380728125572205\n",
      "d_loss:0.6544132988638012\n",
      "g_loss:0.327698290348053\n",
      "Batch:75\n",
      "d_loss_real:0.9587615132331848\n",
      "d_loss_fake:0.0005358433118090034\n",
      "d_loss_wrong:0.750778079032898\n",
      "d_loss:0.6672092372027691\n",
      "g_loss:0.32932502031326294\n",
      "Batch:76\n",
      "d_loss_real:0.9503357410430908\n",
      "d_loss_fake:0.0005885192658752203\n",
      "d_loss_wrong:0.7781942486763\n",
      "d_loss:0.6698635625070892\n",
      "g_loss:0.33148252964019775\n",
      "Batch:77\n",
      "d_loss_real:0.9099727869033813\n",
      "d_loss_fake:0.010616355575621128\n",
      "d_loss_wrong:0.7671850919723511\n",
      "d_loss:0.6494367553386837\n",
      "g_loss:0.3256875276565552\n",
      "Batch:78\n",
      "d_loss_real:0.9450260400772095\n",
      "d_loss_fake:0.006975042633712292\n",
      "d_loss_wrong:0.7366405725479126\n",
      "d_loss:0.658416923834011\n",
      "g_loss:0.3259265720844269\n",
      "Batch:79\n",
      "d_loss_real:0.960824728012085\n",
      "d_loss_fake:0.012272793799638748\n",
      "d_loss_wrong:0.7458520531654358\n",
      "d_loss:0.6699435757473111\n",
      "g_loss:0.3373188376426697\n",
      "Batch:80\n",
      "d_loss_real:0.9611853361129761\n",
      "d_loss_fake:0.002849009819328785\n",
      "d_loss_wrong:0.7630763053894043\n",
      "d_loss:0.6720739968586713\n",
      "g_loss:0.3289749026298523\n",
      "Batch:81\n",
      "d_loss_real:0.9498884677886963\n",
      "d_loss_fake:0.02007068693637848\n",
      "d_loss_wrong:0.7214953303337097\n",
      "d_loss:0.6603357382118702\n",
      "g_loss:0.4504042863845825\n",
      "Batch:82\n",
      "d_loss_real:0.9361515641212463\n",
      "d_loss_fake:0.006981175392866135\n",
      "d_loss_wrong:0.7532392740249634\n",
      "d_loss:0.6581308944150805\n",
      "g_loss:0.37576594948768616\n",
      "Batch:83\n",
      "d_loss_real:0.9465832710266113\n",
      "d_loss_fake:0.0022849200759083033\n",
      "d_loss_wrong:0.7543248534202576\n",
      "d_loss:0.6624440788873471\n",
      "g_loss:0.3267243802547455\n",
      "Batch:84\n",
      "d_loss_real:0.9187905192375183\n",
      "d_loss_fake:0.006433514878153801\n",
      "d_loss_wrong:0.7888392806053162\n",
      "d_loss:0.6582134584896266\n",
      "g_loss:0.3306642174720764\n",
      "Batch:85\n",
      "d_loss_real:0.9694790840148926\n",
      "d_loss_fake:0.003221639897674322\n",
      "d_loss_wrong:0.7104704976081848\n",
      "d_loss:0.6631625763839111\n",
      "g_loss:0.33441662788391113\n",
      "Batch:86\n",
      "d_loss_real:0.9652076959609985\n",
      "d_loss_fake:0.005067319609224796\n",
      "d_loss_wrong:0.7196006178855896\n",
      "d_loss:0.6637708323542029\n",
      "g_loss:0.32741937041282654\n",
      "Batch:87\n",
      "d_loss_real:0.9412848949432373\n",
      "d_loss_fake:0.0019193565240129828\n",
      "d_loss_wrong:0.736542820930481\n",
      "d_loss:0.6552579918352421\n",
      "g_loss:0.32557058334350586\n",
      "Batch:88\n",
      "d_loss_real:0.9247055649757385\n",
      "d_loss_fake:0.002018481492996216\n",
      "d_loss_wrong:0.736817479133606\n",
      "d_loss:0.6470617726445198\n",
      "g_loss:0.32548338174819946\n",
      "Batch:89\n",
      "d_loss_real:0.9158803224563599\n",
      "d_loss_fake:0.0012407833710312843\n",
      "d_loss_wrong:0.7418187856674194\n",
      "d_loss:0.6437050534877926\n",
      "g_loss:0.32589811086654663\n",
      "Batch:90\n",
      "d_loss_real:0.93421870470047\n",
      "d_loss_fake:0.0013953936286270618\n",
      "d_loss_wrong:0.7619697451591492\n",
      "d_loss:0.657950637047179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_loss:0.32529711723327637\n",
      "Batch:91\n",
      "d_loss_real:0.91948002576828\n",
      "d_loss_fake:0.0013579782098531723\n",
      "d_loss_wrong:0.7293640375137329\n",
      "d_loss:0.6424205168150365\n",
      "g_loss:0.3253640830516815\n",
      "Batch:92\n",
      "d_loss_real:0.9251675605773926\n",
      "d_loss_fake:0.0011425771517679095\n",
      "d_loss_wrong:0.7350002527236938\n",
      "d_loss:0.6466194877575617\n",
      "g_loss:0.3253057599067688\n",
      "Batch:93\n",
      "d_loss_real:0.9127361178398132\n",
      "d_loss_fake:0.0011739622568711638\n",
      "d_loss_wrong:0.7725497484207153\n",
      "d_loss:0.6497989865893032\n",
      "g_loss:0.32541149854660034\n",
      "Batch:94\n",
      "d_loss_real:0.9415727853775024\n",
      "d_loss_fake:0.000544748967513442\n",
      "d_loss_wrong:0.7279300093650818\n",
      "d_loss:0.6529050822719\n",
      "g_loss:0.32533982396125793\n",
      "Batch:95\n",
      "d_loss_real:0.9358800649642944\n",
      "d_loss_fake:0.0011678000446408987\n",
      "d_loss_wrong:0.7683361768722534\n",
      "d_loss:0.6603160267113708\n",
      "g_loss:0.32524192333221436\n",
      "Batch:96\n",
      "d_loss_real:0.9626937508583069\n",
      "d_loss_fake:0.0011251483811065555\n",
      "d_loss_wrong:0.7146021127700806\n",
      "d_loss:0.6602786907169502\n",
      "g_loss:0.32533442974090576\n",
      "Batch:97\n",
      "d_loss_real:0.9399254322052002\n",
      "d_loss_fake:0.0009772172197699547\n",
      "d_loss_wrong:0.8047317266464233\n",
      "d_loss:0.6713899520691484\n",
      "g_loss:0.3251449465751648\n",
      "Batch:98\n",
      "d_loss_real:0.8919169902801514\n",
      "d_loss_fake:0.0008875280618667603\n",
      "d_loss_wrong:0.7605330348014832\n",
      "d_loss:0.6363136358559132\n",
      "g_loss:0.32532092928886414\n",
      "Batch:99\n",
      "d_loss_real:0.9255595207214355\n",
      "d_loss_fake:0.0011950983898714185\n",
      "d_loss_wrong:0.7274148464202881\n",
      "d_loss:0.6449322465632576\n",
      "g_loss:0.325336754322052\n",
      "Batch:100\n",
      "d_loss_real:0.9578328132629395\n",
      "d_loss_fake:0.0008925257716327906\n",
      "d_loss_wrong:0.7102388143539429\n",
      "d_loss:0.6566992416628636\n",
      "g_loss:0.32514989376068115\n",
      "Batch:101\n",
      "d_loss_real:0.9215989112854004\n",
      "d_loss_fake:0.0010026446543633938\n",
      "d_loss_wrong:0.7344252467155457\n",
      "d_loss:0.6446564284851775\n",
      "g_loss:0.3254556357860565\n",
      "Batch:102\n",
      "d_loss_real:0.9018860459327698\n",
      "d_loss_fake:0.000695051858201623\n",
      "d_loss_wrong:0.7420746684074402\n",
      "d_loss:0.6366354530327953\n",
      "g_loss:0.3254600167274475\n",
      "Batch:103\n",
      "d_loss_real:0.9182695746421814\n",
      "d_loss_fake:0.0006836792454123497\n",
      "d_loss_wrong:0.7770128846168518\n",
      "d_loss:0.6535589282866567\n",
      "g_loss:0.3251330554485321\n",
      "Batch:104\n",
      "d_loss_real:0.9384146332740784\n",
      "d_loss_fake:0.0007500451756641269\n",
      "d_loss_wrong:0.6790637969970703\n",
      "d_loss:0.6391607771802228\n",
      "g_loss:0.3251785337924957\n",
      "Batch:105\n",
      "d_loss_real:0.9114174842834473\n",
      "d_loss_fake:0.000768754631280899\n",
      "d_loss_wrong:0.7321263551712036\n",
      "d_loss:0.6389325195923448\n",
      "g_loss:0.32521092891693115\n",
      "Batch:106\n",
      "d_loss_real:0.9036061763763428\n",
      "d_loss_fake:0.0009361195843666792\n",
      "d_loss_wrong:0.7252774834632874\n",
      "d_loss:0.6333564889500849\n",
      "g_loss:0.32512491941452026\n",
      "Batch:107\n",
      "d_loss_real:0.9254442453384399\n",
      "d_loss_fake:0.0006663771346211433\n",
      "d_loss_wrong:0.7185772657394409\n",
      "d_loss:0.6425330333877355\n",
      "g_loss:0.3251575529575348\n",
      "Batch:108\n",
      "d_loss_real:0.9057039022445679\n",
      "d_loss_fake:0.0007522579981014132\n",
      "d_loss_wrong:0.7328068017959595\n",
      "d_loss:0.6362417160707992\n",
      "g_loss:0.3252083361148834\n",
      "Batch:109\n",
      "d_loss_real:0.9084552526473999\n",
      "d_loss_fake:0.0007171997567638755\n",
      "d_loss_wrong:0.7059101462364197\n",
      "d_loss:0.6308844628219958\n",
      "g_loss:0.32526087760925293\n",
      "Batch:110\n",
      "d_loss_real:0.9191946983337402\n",
      "d_loss_fake:0.0009907515486702323\n",
      "d_loss_wrong:0.7202581763267517\n",
      "d_loss:0.6399095811357256\n",
      "g_loss:0.3255319595336914\n",
      "Batch:111\n",
      "d_loss_real:0.884326696395874\n",
      "d_loss_fake:0.0008609862998127937\n",
      "d_loss_wrong:0.7429159879684448\n",
      "d_loss:0.6281075917650014\n",
      "g_loss:0.32568153738975525\n",
      "Batch:112\n",
      "d_loss_real:0.9057597517967224\n",
      "d_loss_fake:0.0008335582679137588\n",
      "d_loss_wrong:0.7266280055046082\n",
      "d_loss:0.6347452668414917\n",
      "g_loss:0.32605984807014465\n",
      "Batch:113\n",
      "d_loss_real:0.9188181161880493\n",
      "d_loss_fake:0.0008912792545743287\n",
      "d_loss_wrong:0.7068856954574585\n",
      "d_loss:0.6363533017720329\n",
      "g_loss:0.32655131816864014\n",
      "Batch:114\n",
      "d_loss_real:0.9106640219688416\n",
      "d_loss_fake:0.0007020666962489486\n",
      "d_loss_wrong:0.7194392681121826\n",
      "d_loss:0.6353673446865287\n",
      "g_loss:0.32526037096977234\n",
      "Batch:115\n",
      "d_loss_real:0.8916981816291809\n",
      "d_loss_fake:0.0006693440373055637\n",
      "d_loss_wrong:0.7156845927238464\n",
      "d_loss:0.6249375750048785\n",
      "g_loss:0.32549092173576355\n",
      "Batch:116\n",
      "d_loss_real:0.9147355556488037\n",
      "d_loss_fake:0.0006406077882274985\n",
      "d_loss_wrong:0.742379367351532\n",
      "d_loss:0.6431227716093417\n",
      "g_loss:0.325338214635849\n",
      "Batch:117\n",
      "d_loss_real:0.893720805644989\n",
      "d_loss_fake:0.0005712414276786149\n",
      "d_loss_wrong:0.70895916223526\n",
      "d_loss:0.6242430037382292\n",
      "g_loss:0.3251507878303528\n",
      "Batch:118\n",
      "d_loss_real:0.9158411026000977\n",
      "d_loss_fake:0.0010154964402318\n",
      "d_loss_wrong:0.7183642983436584\n",
      "d_loss:0.6377654999960214\n",
      "g_loss:0.3251519203186035\n",
      "Batch:119\n",
      "d_loss_real:0.8800250887870789\n",
      "d_loss_fake:0.0009016108815558255\n",
      "d_loss_wrong:0.743671178817749\n",
      "d_loss:0.6261557418183656\n",
      "g_loss:0.32512834668159485\n",
      "Batch:120\n",
      "d_loss_real:0.9036338925361633\n",
      "d_loss_fake:0.0006860917201265693\n",
      "d_loss_wrong:0.7534512877464294\n",
      "d_loss:0.6403512911347207\n",
      "g_loss:0.32529041171073914\n",
      "Batch:121\n",
      "d_loss_real:0.9242099523544312\n",
      "d_loss_fake:0.0007195232319645584\n",
      "d_loss_wrong:0.691810131072998\n",
      "d_loss:0.6352373897534562\n",
      "g_loss:0.32587212324142456\n",
      "Batch:122\n",
      "d_loss_real:0.9140334129333496\n",
      "d_loss_fake:0.0006017604027874768\n",
      "d_loss_wrong:0.7134929299354553\n",
      "d_loss:0.6355403790512355\n",
      "g_loss:0.3253585696220398\n",
      "Batch:123\n",
      "d_loss_real:0.911989688873291\n",
      "d_loss_fake:0.0004962638486176729\n",
      "d_loss_wrong:0.7423417568206787\n",
      "d_loss:0.6417043496039696\n",
      "g_loss:0.3252471089363098\n",
      "Batch:124\n",
      "d_loss_real:0.9011056423187256\n",
      "d_loss_fake:0.0005908614257350564\n",
      "d_loss_wrong:0.7115292549133301\n",
      "d_loss:0.6285828502441291\n",
      "g_loss:0.3257034420967102\n",
      "Batch:125\n",
      "d_loss_real:0.9062443375587463\n",
      "d_loss_fake:0.0006699259392917156\n",
      "d_loss_wrong:0.7077975869178772\n",
      "d_loss:0.6302390469936654\n",
      "g_loss:0.3254832625389099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Epoch is: 1\n",
      "Number of batches 125\n",
      "Batch:1\n",
      "d_loss_real:0.9143885374069214\n",
      "d_loss_fake:0.000812365033198148\n",
      "d_loss_wrong:0.695991575717926\n",
      "d_loss:0.6313952538912417\n",
      "g_loss:0.3251771330833435\n",
      "Batch:2\n",
      "d_loss_real:0.9000163078308105\n",
      "d_loss_fake:0.0007702287985011935\n",
      "d_loss_wrong:0.7240316271781921\n",
      "d_loss:0.6312086179095786\n",
      "g_loss:0.32513007521629333\n",
      "Batch:3\n",
      "d_loss_real:0.8821622133255005\n",
      "d_loss_fake:0.0009162353817373514\n",
      "d_loss_wrong:0.7130714058876038\n",
      "d_loss:0.6195780169800855\n",
      "g_loss:0.3251776397228241\n",
      "Batch:4\n",
      "d_loss_real:0.9051389694213867\n",
      "d_loss_fake:0.0006178886396810412\n",
      "d_loss_wrong:0.7535851001739502\n",
      "d_loss:0.6411202319141012\n",
      "g_loss:0.32512980699539185\n",
      "Batch:5\n",
      "d_loss_real:0.9172151684761047\n",
      "d_loss_fake:0.0007239290862344205\n",
      "d_loss_wrong:0.7349065542221069\n",
      "d_loss:0.6425152050651377\n",
      "g_loss:0.3255003094673157\n",
      "Batch:6\n",
      "d_loss_real:0.9067434668540955\n",
      "d_loss_fake:0.0006495437701232731\n",
      "d_loss_wrong:0.7141315937042236\n",
      "d_loss:0.6320670177956345\n",
      "g_loss:0.3261088728904724\n",
      "Batch:7\n",
      "d_loss_real:0.9104491472244263\n",
      "d_loss_fake:0.000654462375678122\n",
      "d_loss_wrong:0.7120537161827087\n",
      "d_loss:0.6334016182518099\n",
      "g_loss:0.3256833553314209\n",
      "Batch:8\n",
      "d_loss_real:0.8888973593711853\n",
      "d_loss_fake:0.0005594125832431018\n",
      "d_loss_wrong:0.7134314179420471\n",
      "d_loss:0.6229463873169152\n",
      "g_loss:0.32530078291893005\n",
      "Batch:9\n",
      "d_loss_real:0.8944038152694702\n",
      "d_loss_fake:0.0007166486466303468\n",
      "d_loss_wrong:0.7099299430847168\n",
      "d_loss:0.6248635555675719\n",
      "g_loss:0.3254150152206421\n",
      "Batch:10\n",
      "d_loss_real:0.9088445901870728\n",
      "d_loss_fake:0.0008245480130426586\n",
      "d_loss_wrong:0.7131317853927612\n",
      "d_loss:0.6329113784449873\n",
      "g_loss:0.32526877522468567\n",
      "Batch:11\n",
      "d_loss_real:0.8949589729309082\n",
      "d_loss_fake:0.0007635395741090178\n",
      "d_loss_wrong:0.7134211659431458\n",
      "d_loss:0.6260256628447678\n",
      "g_loss:0.32524558901786804\n",
      "Batch:12\n",
      "d_loss_real:0.8784037828445435\n",
      "d_loss_fake:0.0008122800500132143\n",
      "d_loss_wrong:0.7278383374214172\n",
      "d_loss:0.6213645457901293\n",
      "g_loss:0.3251277804374695\n",
      "Batch:13\n",
      "d_loss_real:0.9074836373329163\n",
      "d_loss_fake:0.0010457003954797983\n",
      "d_loss_wrong:0.6857087016105652\n",
      "d_loss:0.6254304191679694\n",
      "g_loss:0.3251727819442749\n",
      "Batch:14\n",
      "d_loss_real:0.8897498846054077\n",
      "d_loss_fake:0.000541375600732863\n",
      "d_loss_wrong:0.7264983654022217\n",
      "d_loss:0.6266348775534425\n",
      "g_loss:0.32558077573776245\n",
      "Batch:15\n",
      "d_loss_real:0.9079872369766235\n",
      "d_loss_fake:0.0008724457584321499\n",
      "d_loss_wrong:0.7045309543609619\n",
      "d_loss:0.6303444685181603\n",
      "g_loss:0.32676300406455994\n",
      "Batch:16\n",
      "d_loss_real:0.8942103385925293\n",
      "d_loss_fake:0.0009560459293425083\n",
      "d_loss_wrong:0.7094847559928894\n",
      "d_loss:0.6247153697768226\n",
      "g_loss:0.3288489282131195\n",
      "Batch:17\n",
      "d_loss_real:0.882476806640625\n",
      "d_loss_fake:0.0008602434536442161\n",
      "d_loss_wrong:0.7089136242866516\n",
      "d_loss:0.6186818702553865\n",
      "g_loss:0.3256654739379883\n",
      "Batch:18\n",
      "d_loss_real:0.9202353954315186\n",
      "d_loss_fake:0.0018664326053112745\n",
      "d_loss_wrong:0.7199795842170715\n",
      "d_loss:0.640579201921355\n",
      "g_loss:0.3438635766506195\n",
      "Batch:19\n",
      "d_loss_real:0.8932967185974121\n",
      "d_loss_fake:0.010610630735754967\n",
      "d_loss_wrong:0.7063540816307068\n",
      "d_loss:0.6258895373903215\n",
      "g_loss:0.3262476325035095\n",
      "Batch:20\n",
      "d_loss_real:0.9051992893218994\n",
      "d_loss_fake:0.00980015005916357\n",
      "d_loss_wrong:0.6948735117912292\n",
      "d_loss:0.6287680601235479\n",
      "g_loss:0.45299994945526123\n",
      "Batch:21\n",
      "d_loss_real:0.8951179385185242\n",
      "d_loss_fake:0.16208353638648987\n",
      "d_loss_wrong:0.7150886654853821\n",
      "d_loss:0.6668520197272301\n",
      "g_loss:1.6926062107086182\n",
      "Batch:22\n",
      "d_loss_real:0.9950509667396545\n",
      "d_loss_fake:0.28044360876083374\n",
      "d_loss_wrong:0.5599240064620972\n",
      "d_loss:0.70761738717556\n",
      "g_loss:5.518375396728516\n",
      "Batch:23\n",
      "d_loss_real:1.198940396308899\n",
      "d_loss_fake:0.2570079565048218\n",
      "d_loss_wrong:0.5545271635055542\n",
      "d_loss:0.8023539781570435\n",
      "g_loss:4.653683662414551\n",
      "Batch:24\n",
      "d_loss_real:1.2688454389572144\n",
      "d_loss_fake:0.08231480419635773\n",
      "d_loss_wrong:0.6205618977546692\n",
      "d_loss:0.8101418949663639\n",
      "g_loss:1.137961745262146\n",
      "Batch:25\n",
      "d_loss_real:1.0417042970657349\n",
      "d_loss_fake:0.26651549339294434\n",
      "d_loss_wrong:0.6156392693519592\n",
      "d_loss:0.7413908392190933\n",
      "g_loss:5.920567512512207\n",
      "Batch:26\n",
      "d_loss_real:1.1100144386291504\n",
      "d_loss_fake:0.013323335908353329\n",
      "d_loss_wrong:0.6971137523651123\n",
      "d_loss:0.7326164913829416\n",
      "g_loss:1.9837229251861572\n",
      "Batch:27\n",
      "d_loss_real:1.0726463794708252\n",
      "d_loss_fake:0.7754067182540894\n",
      "d_loss_wrong:0.43791693449020386\n",
      "d_loss:0.8396541029214859\n",
      "g_loss:12.896207809448242\n",
      "Batch:28\n",
      "d_loss_real:2.2541003227233887\n",
      "d_loss_fake:0.019635368138551712\n",
      "d_loss_wrong:0.6749242544174194\n",
      "d_loss:1.3006900670006871\n",
      "g_loss:4.791350364685059\n",
      "Batch:29\n",
      "d_loss_real:1.085407018661499\n",
      "d_loss_fake:0.36043620109558105\n",
      "d_loss_wrong:0.6546357870101929\n",
      "d_loss:0.796471506357193\n",
      "g_loss:1.7406225204467773\n",
      "Batch:30\n",
      "d_loss_real:1.380173921585083\n",
      "d_loss_fake:0.017837923020124435\n",
      "d_loss_wrong:0.6119146943092346\n",
      "d_loss:0.8475251151248813\n",
      "g_loss:0.34605932235717773\n",
      "Batch:31\n",
      "d_loss_real:1.117126226425171\n",
      "d_loss_fake:0.02087775617837906\n",
      "d_loss_wrong:0.7340067625045776\n",
      "d_loss:0.7472842428833246\n",
      "g_loss:0.3379911184310913\n",
      "Batch:32\n",
      "d_loss_real:0.9986320734024048\n",
      "d_loss_fake:0.007787160575389862\n",
      "d_loss_wrong:0.7366999387741089\n",
      "d_loss:0.6854378115385771\n",
      "g_loss:0.3356749415397644\n",
      "Batch:33\n",
      "d_loss_real:1.0037007331848145\n",
      "d_loss_fake:0.007702657952904701\n",
      "d_loss_wrong:0.7142152786254883\n",
      "d_loss:0.6823298507370055\n",
      "g_loss:0.3520042300224304\n",
      "Batch:34\n",
      "d_loss_real:0.9330592751502991\n",
      "d_loss_fake:0.00722398841753602\n",
      "d_loss_wrong:0.7506406307220459\n",
      "d_loss:0.655995792360045\n",
      "g_loss:0.33206912875175476\n",
      "Batch:35\n",
      "d_loss_real:0.9710314273834229\n",
      "d_loss_fake:0.00665940111503005\n",
      "d_loss_wrong:0.7366610765457153\n",
      "d_loss:0.6713458331068978\n",
      "g_loss:0.4046381115913391\n",
      "Batch:36\n",
      "d_loss_real:0.9473304748535156\n",
      "d_loss_fake:0.00817061960697174\n",
      "d_loss_wrong:0.7105661034584045\n",
      "d_loss:0.6533494181931019\n",
      "g_loss:0.4070705771446228\n",
      "Batch:37\n",
      "d_loss_real:0.9863450527191162\n",
      "d_loss_fake:0.007577213458716869\n",
      "d_loss_wrong:0.7309132814407349\n",
      "d_loss:0.677795150084421\n",
      "g_loss:0.3486118018627167\n",
      "Batch:38\n",
      "d_loss_real:0.9217045307159424\n",
      "d_loss_fake:0.007857462391257286\n",
      "d_loss_wrong:0.7417151927947998\n",
      "d_loss:0.6482454291544855\n",
      "g_loss:0.3299277424812317\n",
      "Batch:39\n",
      "d_loss_real:0.9236882925033569\n",
      "d_loss_fake:0.005414166487753391\n",
      "d_loss_wrong:0.7072322368621826\n",
      "d_loss:0.6400057470891625\n",
      "g_loss:0.3554539382457733\n",
      "Batch:40\n",
      "d_loss_real:0.9782668352127075\n",
      "d_loss_fake:0.006204834207892418\n",
      "d_loss_wrong:0.6785840392112732\n",
      "d_loss:0.6603306359611452\n",
      "g_loss:0.33140629529953003\n",
      "Batch:41\n",
      "d_loss_real:0.9238106608390808\n",
      "d_loss_fake:0.004899625200778246\n",
      "d_loss_wrong:0.7168521881103516\n",
      "d_loss:0.6423432837473229\n",
      "g_loss:0.34577032923698425\n",
      "Batch:42\n",
      "d_loss_real:0.9237464666366577\n",
      "d_loss_fake:0.005563060287386179\n",
      "d_loss_wrong:0.7213621139526367\n",
      "d_loss:0.6436045268783346\n",
      "g_loss:0.336201548576355\n",
      "Batch:43\n",
      "d_loss_real:0.929290771484375\n",
      "d_loss_fake:0.004641260951757431\n",
      "d_loss_wrong:0.7244049906730652\n",
      "d_loss:0.6469069486483932\n",
      "g_loss:0.3603689670562744\n",
      "Batch:44\n",
      "d_loss_real:0.9311060905456543\n",
      "d_loss_fake:0.00608536321669817\n",
      "d_loss_wrong:0.6969614028930664\n",
      "d_loss:0.6413147368002683\n",
      "g_loss:0.33926552534103394\n",
      "Batch:45\n",
      "d_loss_real:0.8985346555709839\n",
      "d_loss_fake:0.006323663052171469\n",
      "d_loss_wrong:0.7284383177757263\n",
      "d_loss:0.6329578229924664\n",
      "g_loss:0.35792213678359985\n",
      "Batch:46\n",
      "d_loss_real:0.9530109167098999\n",
      "d_loss_fake:0.007283473387360573\n",
      "d_loss_wrong:0.6637707948684692\n",
      "d_loss:0.6442690254189074\n",
      "g_loss:0.3341915011405945\n",
      "Batch:47\n",
      "d_loss_real:0.902627170085907\n",
      "d_loss_fake:0.007889699190855026\n",
      "d_loss_wrong:0.7486394047737122\n",
      "d_loss:0.6404458610340953\n",
      "g_loss:0.3467322587966919\n",
      "Batch:48\n",
      "d_loss_real:0.8954741358757019\n",
      "d_loss_fake:0.007242907769978046\n",
      "d_loss_wrong:0.6873067021369934\n",
      "d_loss:0.6213744704145938\n",
      "g_loss:0.3497771620750427\n",
      "Batch:49\n",
      "d_loss_real:0.9276022911071777\n",
      "d_loss_fake:0.00963724497705698\n",
      "d_loss_wrong:0.7091599106788635\n",
      "d_loss:0.643500434467569\n",
      "g_loss:0.35320189595222473\n",
      "Batch:50\n",
      "d_loss_real:0.9199234843254089\n",
      "d_loss_fake:0.009058289229869843\n",
      "d_loss_wrong:0.7029101252555847\n",
      "d_loss:0.6379538457840681\n",
      "g_loss:0.4212268590927124\n",
      "Batch:51\n",
      "d_loss_real:0.9231642484664917\n",
      "d_loss_fake:0.013197038322687149\n",
      "d_loss_wrong:0.6833786964416504\n",
      "d_loss:0.6357260579243302\n",
      "g_loss:0.34947699308395386\n",
      "Batch:52\n",
      "d_loss_real:0.9353823661804199\n",
      "d_loss_fake:0.010996362194418907\n",
      "d_loss_wrong:0.6848850250244141\n",
      "d_loss:0.6416615298949182\n",
      "g_loss:0.3583070635795593\n",
      "Batch:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_real:0.911203920841217\n",
      "d_loss_fake:0.011363688856363297\n",
      "d_loss_wrong:0.7095561027526855\n",
      "d_loss:0.6358319083228707\n",
      "g_loss:0.42156779766082764\n",
      "Batch:54\n",
      "d_loss_real:0.9086926579475403\n",
      "d_loss_fake:0.00893958006054163\n",
      "d_loss_wrong:0.6892985105514526\n",
      "d_loss:0.6289058516267687\n",
      "g_loss:0.43514758348464966\n",
      "Batch:55\n",
      "d_loss_real:0.9176110029220581\n",
      "d_loss_fake:0.015354959294199944\n",
      "d_loss_wrong:0.6978826522827148\n",
      "d_loss:0.6371149043552577\n",
      "g_loss:0.4118709862232208\n",
      "Batch:56\n",
      "d_loss_real:0.937019407749176\n",
      "d_loss_fake:0.017294282093644142\n",
      "d_loss_wrong:0.6707154512405396\n",
      "d_loss:0.6405121372081339\n",
      "g_loss:0.3942197561264038\n",
      "Batch:57\n",
      "d_loss_real:0.8898768424987793\n",
      "d_loss_fake:0.011292858049273491\n",
      "d_loss_wrong:0.7093392014503479\n",
      "d_loss:0.625096436124295\n",
      "g_loss:0.3688235580921173\n",
      "Batch:58\n",
      "d_loss_real:0.9127051830291748\n",
      "d_loss_fake:0.00896555557847023\n",
      "d_loss_wrong:0.6910379528999329\n",
      "d_loss:0.6313534686341882\n",
      "g_loss:0.3580490052700043\n",
      "Batch:59\n",
      "d_loss_real:0.9056390523910522\n",
      "d_loss_fake:0.00910321343690157\n",
      "d_loss_wrong:0.6897085309028625\n",
      "d_loss:0.6275224622804672\n",
      "g_loss:0.37073802947998047\n",
      "Batch:60\n",
      "d_loss_real:0.9233946204185486\n",
      "d_loss_fake:0.009951946325600147\n",
      "d_loss_wrong:0.6704082489013672\n",
      "d_loss:0.6317873590160161\n",
      "g_loss:0.36875981092453003\n",
      "Batch:61\n",
      "d_loss_real:0.9110821485519409\n",
      "d_loss_fake:0.011823181994259357\n",
      "d_loss_wrong:0.7124468088150024\n",
      "d_loss:0.6366085719782859\n",
      "g_loss:0.3541778028011322\n",
      "Batch:62\n",
      "d_loss_real:0.9129855632781982\n",
      "d_loss_fake:0.01229449175298214\n",
      "d_loss_wrong:0.6912351846694946\n",
      "d_loss:0.6323752007447183\n",
      "g_loss:0.357593297958374\n",
      "Batch:63\n",
      "d_loss_real:0.8835233449935913\n",
      "d_loss_fake:0.0084722600877285\n",
      "d_loss_wrong:0.7086175084114075\n",
      "d_loss:0.6210341146215796\n",
      "g_loss:0.3643208146095276\n",
      "Batch:64\n",
      "d_loss_real:0.9072312116622925\n",
      "d_loss_fake:0.007742011919617653\n",
      "d_loss_wrong:0.6863954067230225\n",
      "d_loss:0.6271499604918063\n",
      "g_loss:0.35728132724761963\n",
      "Batch:65\n",
      "d_loss_real:0.9112840890884399\n",
      "d_loss_fake:0.010301697067916393\n",
      "d_loss_wrong:0.693010687828064\n",
      "d_loss:0.6314701407682151\n",
      "g_loss:0.3597395420074463\n",
      "Batch:66\n",
      "d_loss_real:0.899854302406311\n",
      "d_loss_fake:0.009322043508291245\n",
      "d_loss_wrong:0.6936078667640686\n",
      "d_loss:0.6256596287712455\n",
      "g_loss:0.39637213945388794\n",
      "Batch:67\n",
      "d_loss_real:0.9211986064910889\n",
      "d_loss_fake:0.010581159964203835\n",
      "d_loss_wrong:0.6756421327590942\n",
      "d_loss:0.632155126426369\n",
      "g_loss:0.3574039936065674\n",
      "Batch:68\n",
      "d_loss_real:0.8896563649177551\n",
      "d_loss_fake:0.007598115596920252\n",
      "d_loss_wrong:0.7050381302833557\n",
      "d_loss:0.6229872439289466\n",
      "g_loss:0.4014613926410675\n",
      "Batch:69\n",
      "d_loss_real:0.9078385233879089\n",
      "d_loss_fake:0.007295726332813501\n",
      "d_loss_wrong:0.6912808418273926\n",
      "d_loss:0.628563403734006\n",
      "g_loss:0.38087010383605957\n",
      "Batch:70\n",
      "d_loss_real:0.908400297164917\n",
      "d_loss_fake:0.008994289673864841\n",
      "d_loss_wrong:0.6996331214904785\n",
      "d_loss:0.6313570013735443\n",
      "g_loss:0.38432949781417847\n",
      "Batch:71\n",
      "d_loss_real:0.9051594734191895\n",
      "d_loss_fake:0.006814721971750259\n",
      "d_loss_wrong:0.6822767853736877\n",
      "d_loss:0.6248526135459542\n",
      "g_loss:0.3813980221748352\n",
      "Batch:72\n",
      "d_loss_real:0.8889767527580261\n",
      "d_loss_fake:0.005816091783344746\n",
      "d_loss_wrong:0.710556149482727\n",
      "d_loss:0.623581436695531\n",
      "g_loss:0.3936570882797241\n",
      "Batch:73\n",
      "d_loss_real:0.8767284154891968\n",
      "d_loss_fake:0.004753586370497942\n",
      "d_loss_wrong:0.7012115716934204\n",
      "d_loss:0.614855497260578\n",
      "g_loss:0.401561439037323\n",
      "Batch:74\n",
      "d_loss_real:0.8913426995277405\n",
      "d_loss_fake:0.005043982993811369\n",
      "d_loss_wrong:0.6867616772651672\n",
      "d_loss:0.6186227648286149\n",
      "g_loss:0.38638418912887573\n",
      "Batch:75\n",
      "d_loss_real:0.9218320846557617\n",
      "d_loss_fake:0.005804124288260937\n",
      "d_loss_wrong:0.6909324526786804\n",
      "d_loss:0.6351001865696162\n",
      "g_loss:0.37709349393844604\n",
      "Batch:76\n",
      "d_loss_real:0.9233911037445068\n",
      "d_loss_fake:0.007315994706004858\n",
      "d_loss_wrong:0.6903732419013977\n",
      "d_loss:0.6361178610241041\n",
      "g_loss:0.3599247932434082\n",
      "Batch:77\n",
      "d_loss_real:0.8702072501182556\n",
      "d_loss_fake:0.004317739047110081\n",
      "d_loss_wrong:0.713686466217041\n",
      "d_loss:0.6146046763751656\n",
      "g_loss:0.36663275957107544\n",
      "Batch:78\n",
      "d_loss_real:0.8817416429519653\n",
      "d_loss_fake:0.004324387293308973\n",
      "d_loss_wrong:0.6849371790885925\n",
      "d_loss:0.613186213071458\n",
      "g_loss:0.3749144673347473\n",
      "Batch:79\n",
      "d_loss_real:0.8889145851135254\n",
      "d_loss_fake:0.0037300940603017807\n",
      "d_loss_wrong:0.7082609534263611\n",
      "d_loss:0.6224550544284284\n",
      "g_loss:0.37003079056739807\n",
      "Batch:80\n",
      "d_loss_real:0.8804455995559692\n",
      "d_loss_fake:0.0027867588214576244\n",
      "d_loss_wrong:0.7112973928451538\n",
      "d_loss:0.6187438376946375\n",
      "g_loss:0.3474574089050293\n",
      "Batch:81\n",
      "d_loss_real:0.8992211222648621\n",
      "d_loss_fake:0.0033766399137675762\n",
      "d_loss_wrong:0.6751860976219177\n",
      "d_loss:0.6192512455163524\n",
      "g_loss:0.34863561391830444\n",
      "Batch:82\n",
      "d_loss_real:0.8995858430862427\n",
      "d_loss_fake:0.004169819410890341\n",
      "d_loss_wrong:0.692659318447113\n",
      "d_loss:0.6240002060076222\n",
      "g_loss:0.3420398235321045\n",
      "Batch:83\n",
      "d_loss_real:0.8837783336639404\n",
      "d_loss_fake:0.0028707885649055243\n",
      "d_loss_wrong:0.7027606964111328\n",
      "d_loss:0.6182970380759798\n",
      "g_loss:0.3360420763492584\n",
      "Batch:84\n",
      "d_loss_real:0.8678018450737\n",
      "d_loss_fake:0.002764903474599123\n",
      "d_loss_wrong:0.725914478302002\n",
      "d_loss:0.6160707679810002\n",
      "g_loss:0.3497985601425171\n",
      "Batch:85\n",
      "d_loss_real:0.930733859539032\n",
      "d_loss_fake:0.004190253093838692\n",
      "d_loss_wrong:0.6661231517791748\n",
      "d_loss:0.6329452809877694\n",
      "g_loss:0.3347227871417999\n",
      "Batch:86\n",
      "d_loss_real:0.8856624364852905\n",
      "d_loss_fake:0.0032481448724865913\n",
      "d_loss_wrong:0.6904496550559998\n",
      "d_loss:0.6162556682247669\n",
      "g_loss:0.3318801522254944\n",
      "Batch:87\n",
      "d_loss_real:0.8884720802307129\n",
      "d_loss_fake:0.003130451776087284\n",
      "d_loss_wrong:0.6886715292930603\n",
      "d_loss:0.6171865353826433\n",
      "g_loss:0.3315814435482025\n",
      "Batch:88\n",
      "d_loss_real:0.8847346305847168\n",
      "d_loss_fake:0.003985334187746048\n",
      "d_loss_wrong:0.6851628422737122\n",
      "d_loss:0.614654359407723\n",
      "g_loss:0.33774417638778687\n",
      "Batch:89\n",
      "d_loss_real:0.8865123987197876\n",
      "d_loss_fake:0.003161654807627201\n",
      "d_loss_wrong:0.6859313249588013\n",
      "d_loss:0.6155294443015009\n",
      "g_loss:0.34427326917648315\n",
      "Batch:90\n",
      "d_loss_real:0.8694758415222168\n",
      "d_loss_fake:0.0026112059131264687\n",
      "d_loss_wrong:0.718335747718811\n",
      "d_loss:0.6149746591690928\n",
      "g_loss:0.33702540397644043\n",
      "Batch:91\n",
      "d_loss_real:0.8665497303009033\n",
      "d_loss_fake:0.0022850087843835354\n",
      "d_loss_wrong:0.6906548142433167\n",
      "d_loss:0.6065098209073767\n",
      "g_loss:0.34070926904678345\n",
      "Batch:92\n",
      "d_loss_real:0.901594877243042\n",
      "d_loss_fake:0.0025477074086666107\n",
      "d_loss_wrong:0.6783840656280518\n",
      "d_loss:0.6210303818807006\n",
      "g_loss:0.3322921097278595\n",
      "Batch:93\n",
      "d_loss_real:0.8697587847709656\n",
      "d_loss_fake:0.003281994489952922\n",
      "d_loss_wrong:0.7213162779808044\n",
      "d_loss:0.6160289605031721\n",
      "g_loss:0.3354126214981079\n",
      "Batch:94\n",
      "d_loss_real:0.9010226130485535\n",
      "d_loss_fake:0.0021192049607634544\n",
      "d_loss_wrong:0.6850845813751221\n",
      "d_loss:0.6223122531082481\n",
      "g_loss:0.3291648328304291\n",
      "Batch:95\n",
      "d_loss_real:0.8846626281738281\n",
      "d_loss_fake:0.0029608705081045628\n",
      "d_loss_wrong:0.7061488628387451\n",
      "d_loss:0.6196087474236265\n",
      "g_loss:0.3305312395095825\n",
      "Batch:96\n",
      "d_loss_real:0.9116158485412598\n",
      "d_loss_fake:0.003383538220077753\n",
      "d_loss_wrong:0.6690570116043091\n",
      "d_loss:0.6239180617267266\n",
      "g_loss:0.33172541856765747\n",
      "Batch:97\n",
      "d_loss_real:0.8915574550628662\n",
      "d_loss_fake:0.002609211951494217\n",
      "d_loss_wrong:0.7306254506111145\n",
      "d_loss:0.6290873931720853\n",
      "g_loss:0.3304835259914398\n",
      "Batch:98\n",
      "d_loss_real:0.8648295402526855\n",
      "d_loss_fake:0.0021484072785824537\n",
      "d_loss_wrong:0.7164285778999329\n",
      "d_loss:0.6120590164209716\n",
      "g_loss:0.3353646993637085\n",
      "Batch:99\n",
      "d_loss_real:0.8702793717384338\n",
      "d_loss_fake:0.003079846501350403\n",
      "d_loss_wrong:0.6923946142196655\n",
      "d_loss:0.6090083010494709\n",
      "g_loss:0.331689715385437\n",
      "Batch:100\n",
      "d_loss_real:0.9035527110099792\n",
      "d_loss_fake:0.0019424569327384233\n",
      "d_loss_wrong:0.6748844385147095\n",
      "d_loss:0.6209830793668516\n",
      "g_loss:0.32626622915267944\n",
      "Batch:101\n",
      "d_loss_real:0.8882103562355042\n",
      "d_loss_fake:0.0033564704935997725\n",
      "d_loss_wrong:0.6875420808792114\n",
      "d_loss:0.6168298159609549\n",
      "g_loss:0.3284645676612854\n",
      "Batch:102\n",
      "d_loss_real:0.8855223059654236\n",
      "d_loss_fake:0.002175619825720787\n",
      "d_loss_wrong:0.6909725666046143\n",
      "d_loss:0.6160481995902956\n",
      "g_loss:0.326790452003479\n",
      "Batch:103\n",
      "d_loss_real:0.8790314197540283\n",
      "d_loss_fake:0.0018893949454650283\n",
      "d_loss_wrong:0.7275860905647278\n",
      "d_loss:0.6218845812545624\n",
      "g_loss:0.3278169631958008\n",
      "Batch:104\n",
      "d_loss_real:0.8644338846206665\n",
      "d_loss_fake:0.0019649434834718704\n",
      "d_loss_wrong:0.6709352135658264\n",
      "d_loss:0.6004419815726578\n",
      "g_loss:0.3278992474079132\n",
      "Batch:105\n",
      "d_loss_real:0.873794674873352\n",
      "d_loss_fake:0.001688696094788611\n",
      "d_loss_wrong:0.6926209330558777\n",
      "d_loss:0.6104747447243426\n",
      "g_loss:0.32766997814178467\n",
      "Batch:106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_real:0.8712641000747681\n",
      "d_loss_fake:0.002251541707664728\n",
      "d_loss_wrong:0.6864813566207886\n",
      "d_loss:0.6078152746194974\n",
      "g_loss:0.3290017247200012\n",
      "Batch:107\n",
      "d_loss_real:0.9058549404144287\n",
      "d_loss_fake:0.0018883259035646915\n",
      "d_loss_wrong:0.6716508269309998\n",
      "d_loss:0.6213122584158555\n",
      "g_loss:0.3281414210796356\n",
      "Batch:108\n",
      "d_loss_real:0.875910222530365\n",
      "d_loss_fake:0.0024689773563295603\n",
      "d_loss_wrong:0.6925395131111145\n",
      "d_loss:0.6117072338820435\n",
      "g_loss:0.3303316831588745\n",
      "Batch:109\n",
      "d_loss_real:0.8662557601928711\n",
      "d_loss_fake:0.0017543314024806023\n",
      "d_loss_wrong:0.6832529902458191\n",
      "d_loss:0.6043797105085105\n",
      "g_loss:0.32772839069366455\n",
      "Batch:110\n",
      "d_loss_real:0.8943773508071899\n",
      "d_loss_fake:0.0021650136914104223\n",
      "d_loss_wrong:0.6774616241455078\n",
      "d_loss:0.6170953348628245\n",
      "g_loss:0.32730960845947266\n",
      "Batch:111\n",
      "d_loss_real:0.847943127155304\n",
      "d_loss_fake:0.0018846007296815515\n",
      "d_loss_wrong:0.7103762030601501\n",
      "d_loss:0.6020367645251099\n",
      "g_loss:0.3278018832206726\n",
      "Batch:112\n",
      "d_loss_real:0.8554396033287048\n",
      "d_loss_fake:0.0015894918469712138\n",
      "d_loss_wrong:0.6981536746025085\n",
      "d_loss:0.6026555932767224\n",
      "g_loss:0.3304142951965332\n",
      "Batch:113\n",
      "d_loss_real:0.8909467458724976\n",
      "d_loss_fake:0.002070852555334568\n",
      "d_loss_wrong:0.6683037877082825\n",
      "d_loss:0.613067033002153\n",
      "g_loss:0.33028656244277954\n",
      "Batch:114\n",
      "d_loss_real:0.886539876461029\n",
      "d_loss_fake:0.0012401808053255081\n",
      "d_loss_wrong:0.6866196990013123\n",
      "d_loss:0.615234908182174\n",
      "g_loss:0.32632482051849365\n",
      "Batch:115\n",
      "d_loss_real:0.8749030828475952\n",
      "d_loss_fake:0.0018167933449149132\n",
      "d_loss_wrong:0.676018238067627\n",
      "d_loss:0.6069102992769331\n",
      "g_loss:0.32643556594848633\n",
      "Batch:116\n",
      "d_loss_real:0.8771201372146606\n",
      "d_loss_fake:0.0012146176304668188\n",
      "d_loss_wrong:0.7056773900985718\n",
      "d_loss:0.61528307053959\n",
      "g_loss:0.32652217149734497\n",
      "Batch:117\n",
      "d_loss_real:0.8616829514503479\n",
      "d_loss_fake:0.0013578284997493029\n",
      "d_loss_wrong:0.6809483766555786\n",
      "d_loss:0.6014180270140059\n",
      "g_loss:0.32682907581329346\n",
      "Batch:118\n",
      "d_loss_real:0.8870524168014526\n",
      "d_loss_fake:0.0020622387528419495\n",
      "d_loss_wrong:0.6796659827232361\n",
      "d_loss:0.6139582637697458\n",
      "g_loss:0.32741349935531616\n",
      "Batch:119\n",
      "d_loss_real:0.8550273776054382\n",
      "d_loss_fake:0.0011474515777081251\n",
      "d_loss_wrong:0.7074949145317078\n",
      "d_loss:0.6046742803300731\n",
      "g_loss:0.32651287317276\n",
      "Batch:120\n",
      "d_loss_real:0.8707079887390137\n",
      "d_loss_fake:0.0014505806611850858\n",
      "d_loss_wrong:0.7090857625007629\n",
      "d_loss:0.6129880801599938\n",
      "g_loss:0.33142760396003723\n",
      "Batch:121\n",
      "d_loss_real:0.887244701385498\n",
      "d_loss_fake:0.0016730079660192132\n",
      "d_loss_wrong:0.6683483123779297\n",
      "d_loss:0.6111276807787362\n",
      "g_loss:0.3291271924972534\n",
      "Batch:122\n",
      "d_loss_real:0.8696517944335938\n",
      "d_loss_fake:0.0014630014775320888\n",
      "d_loss_wrong:0.6867485642433167\n",
      "d_loss:0.6068787886470091\n",
      "g_loss:0.32592812180519104\n",
      "Batch:123\n",
      "d_loss_real:0.8760538101196289\n",
      "d_loss_fake:0.0011072591878473759\n",
      "d_loss_wrong:0.7018058896064758\n",
      "d_loss:0.6137551922583953\n",
      "g_loss:0.3258622884750366\n",
      "Batch:124\n",
      "d_loss_real:0.8789428472518921\n",
      "d_loss_fake:0.0013768116477876902\n",
      "d_loss_wrong:0.6777478456497192\n",
      "d_loss:0.6092525879503228\n",
      "g_loss:0.32680392265319824\n",
      "Batch:125\n",
      "d_loss_real:0.8758023977279663\n",
      "d_loss_fake:0.0018991524120792747\n",
      "d_loss_wrong:0.6762683391571045\n",
      "d_loss:0.6074430717562791\n",
      "g_loss:0.3269067406654358\n",
      "========================================\n",
      "Epoch is: 2\n",
      "Number of batches 125\n",
      "Batch:1\n",
      "d_loss_real:0.8891993761062622\n",
      "d_loss_fake:0.0013727450277656317\n",
      "d_loss_wrong:0.6632709503173828\n",
      "d_loss:0.6107606118894182\n",
      "g_loss:0.3264203667640686\n",
      "Batch:2\n",
      "d_loss_real:0.8679102659225464\n",
      "d_loss_fake:0.0020184372551739216\n",
      "d_loss_wrong:0.6906819939613342\n",
      "d_loss:0.6071302407654002\n",
      "g_loss:0.32703936100006104\n",
      "Batch:3\n",
      "d_loss_real:0.8554853796958923\n",
      "d_loss_fake:0.0018945239717140794\n",
      "d_loss_wrong:0.6863188147544861\n",
      "d_loss:0.5997960245294962\n",
      "g_loss:0.32708439230918884\n",
      "Batch:4\n",
      "d_loss_real:0.8709422945976257\n",
      "d_loss_fake:0.0013759370194748044\n",
      "d_loss_wrong:0.7134845852851868\n",
      "d_loss:0.6141862778749783\n",
      "g_loss:0.3279443681240082\n",
      "Batch:5\n",
      "d_loss_real:0.8755053877830505\n",
      "d_loss_fake:0.0016254513757303357\n",
      "d_loss_wrong:0.7106927633285522\n",
      "d_loss:0.6158322475675959\n",
      "g_loss:0.32697489857673645\n",
      "Batch:6\n",
      "d_loss_real:0.8691721558570862\n",
      "d_loss_fake:0.001558738760650158\n",
      "d_loss_wrong:0.6807153820991516\n",
      "d_loss:0.6051546081434935\n",
      "g_loss:0.3258572518825531\n",
      "Batch:7\n",
      "d_loss_real:0.881229817867279\n",
      "d_loss_fake:0.0011303756618872285\n",
      "d_loss_wrong:0.6779665946960449\n",
      "d_loss:0.6103891515231226\n",
      "g_loss:0.3271222412586212\n",
      "Batch:8\n",
      "d_loss_real:0.8605173826217651\n",
      "d_loss_fake:0.0016008771490305662\n",
      "d_loss_wrong:0.6797662973403931\n",
      "d_loss:0.6006004849332385\n",
      "g_loss:0.3261973261833191\n",
      "Batch:9\n",
      "d_loss_real:0.8748306632041931\n",
      "d_loss_fake:0.0023530791513621807\n",
      "d_loss_wrong:0.6783950328826904\n",
      "d_loss:0.6076023596106097\n",
      "g_loss:0.32768726348876953\n",
      "Batch:10\n",
      "d_loss_real:0.8883771300315857\n",
      "d_loss_fake:0.0016448593232780695\n",
      "d_loss_wrong:0.682681679725647\n",
      "d_loss:0.6152701997780241\n",
      "g_loss:0.3268103003501892\n",
      "Batch:11\n",
      "d_loss_real:0.8786269426345825\n",
      "d_loss_fake:0.0016969600692391396\n",
      "d_loss_wrong:0.6789867281913757\n",
      "d_loss:0.609484393382445\n",
      "g_loss:0.3272092342376709\n",
      "Batch:12\n",
      "d_loss_real:0.8408191204071045\n",
      "d_loss_fake:0.0016368223587051034\n",
      "d_loss_wrong:0.7128286957740784\n",
      "d_loss:0.5990259397367481\n",
      "g_loss:0.3278411030769348\n",
      "Batch:13\n",
      "d_loss_real:0.8673556447029114\n",
      "d_loss_fake:0.001058037276379764\n",
      "d_loss_wrong:0.6708231568336487\n",
      "d_loss:0.6016481208789628\n",
      "g_loss:0.326282262802124\n",
      "Batch:14\n",
      "d_loss_real:0.8755806684494019\n",
      "d_loss_fake:0.000736028072424233\n",
      "d_loss_wrong:0.6877604722976685\n",
      "d_loss:0.6099144593172241\n",
      "g_loss:0.32668566703796387\n",
      "Batch:15\n",
      "d_loss_real:0.8828252553939819\n",
      "d_loss_fake:0.0009978648740798235\n",
      "d_loss_wrong:0.677931547164917\n",
      "d_loss:0.6111449807067402\n",
      "g_loss:0.3276679515838623\n",
      "Batch:16\n",
      "d_loss_real:0.8676584959030151\n",
      "d_loss_fake:0.0017860017251223326\n",
      "d_loss_wrong:0.6795845627784729\n",
      "d_loss:0.6041718890774064\n",
      "g_loss:0.32800644636154175\n",
      "Batch:17\n",
      "d_loss_real:0.851370096206665\n",
      "d_loss_fake:0.0013525632675737143\n",
      "d_loss_wrong:0.6883032917976379\n",
      "d_loss:0.5980990118696354\n",
      "g_loss:0.3269112706184387\n",
      "Batch:18\n",
      "d_loss_real:0.8983776569366455\n",
      "d_loss_fake:0.0016118765342980623\n",
      "d_loss_wrong:0.6796575784683228\n",
      "d_loss:0.619506192218978\n",
      "g_loss:0.3268676996231079\n",
      "Batch:19\n",
      "d_loss_real:0.881161630153656\n",
      "d_loss_fake:0.0015243052039295435\n",
      "d_loss_wrong:0.6854930520057678\n",
      "d_loss:0.6123351543792523\n",
      "g_loss:0.326421856880188\n",
      "Batch:20\n",
      "d_loss_real:0.8694467544555664\n",
      "d_loss_fake:0.0012464607134461403\n",
      "d_loss_wrong:0.6769567131996155\n",
      "d_loss:0.6042741707060486\n",
      "g_loss:0.32610273361206055\n",
      "Batch:21\n",
      "d_loss_real:0.8554749488830566\n",
      "d_loss_fake:0.0011615263065323234\n",
      "d_loss_wrong:0.7191522717475891\n",
      "d_loss:0.6078159239550587\n",
      "g_loss:0.3285773694515228\n",
      "Batch:22\n",
      "d_loss_real:0.8674623966217041\n",
      "d_loss_fake:0.0011892369948327541\n",
      "d_loss_wrong:0.672458827495575\n",
      "d_loss:0.602143214433454\n",
      "g_loss:0.32763126492500305\n",
      "Batch:23\n",
      "d_loss_real:0.8696457147598267\n",
      "d_loss_fake:0.0009605531813576818\n",
      "d_loss_wrong:0.6882153749465942\n",
      "d_loss:0.6071168394119013\n",
      "g_loss:0.3259126543998718\n",
      "Batch:24\n",
      "d_loss_real:0.8699453473091125\n",
      "d_loss_fake:0.0011215778067708015\n",
      "d_loss_wrong:0.6788100600242615\n",
      "d_loss:0.6049555831123143\n",
      "g_loss:0.326292484998703\n",
      "Batch:25\n",
      "d_loss_real:0.8651552796363831\n",
      "d_loss_fake:0.00111799081787467\n",
      "d_loss_wrong:0.6983528733253479\n",
      "d_loss:0.6074453558539972\n",
      "g_loss:0.32672780752182007\n",
      "Batch:26\n",
      "d_loss_real:0.8456201553344727\n",
      "d_loss_fake:0.0010573345934972167\n",
      "d_loss_wrong:0.6981427669525146\n",
      "d_loss:0.5976101030537393\n",
      "g_loss:0.32698047161102295\n",
      "Batch:27\n",
      "d_loss_real:0.8778296113014221\n",
      "d_loss_fake:0.001022118842229247\n",
      "d_loss_wrong:0.6647634506225586\n",
      "d_loss:0.605361198016908\n",
      "g_loss:0.3263683319091797\n",
      "Batch:28\n",
      "d_loss_real:0.8852056860923767\n",
      "d_loss_fake:0.0011356555623933673\n",
      "d_loss_wrong:0.6874497532844543\n",
      "d_loss:0.6147491952579003\n",
      "g_loss:0.32610851526260376\n",
      "Batch:29\n",
      "d_loss_real:0.8754695653915405\n",
      "d_loss_fake:0.0011370135471224785\n",
      "d_loss_wrong:0.6778428554534912\n",
      "d_loss:0.6074797499459237\n",
      "g_loss:0.32600533962249756\n",
      "Batch:30\n",
      "d_loss_real:0.8683338165283203\n",
      "d_loss_fake:0.0016322695882990956\n",
      "d_loss_wrong:0.6778507232666016\n",
      "d_loss:0.6040376564778853\n",
      "g_loss:0.32690978050231934\n",
      "Batch:31\n",
      "d_loss_real:0.8620649576187134\n",
      "d_loss_fake:0.0013240328989923\n",
      "d_loss_wrong:0.688069760799408\n",
      "d_loss:0.6033809272339568\n",
      "g_loss:0.32659900188446045\n",
      "Batch:32\n",
      "d_loss_real:0.8645058870315552\n",
      "d_loss_fake:0.0010242490097880363\n",
      "d_loss_wrong:0.6741493344306946\n",
      "d_loss:0.6010463393758982\n",
      "g_loss:0.3260380029678345\n",
      "Batch:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_real:0.8633173704147339\n",
      "d_loss_fake:0.0009049372747540474\n",
      "d_loss_wrong:0.6871017217636108\n",
      "d_loss:0.6036603499669582\n",
      "g_loss:0.32608723640441895\n",
      "Batch:34\n",
      "d_loss_real:0.8726122975349426\n",
      "d_loss_fake:0.0008007987635210156\n",
      "d_loss_wrong:0.6721591949462891\n",
      "d_loss:0.6045461471949238\n",
      "g_loss:0.3259345293045044\n",
      "Batch:35\n",
      "d_loss_real:0.8602863550186157\n",
      "d_loss_fake:0.0011480950051918626\n",
      "d_loss_wrong:0.7055918574333191\n",
      "d_loss:0.6068281656189356\n",
      "g_loss:0.32682210206985474\n",
      "Batch:36\n",
      "d_loss_real:0.8627626895904541\n",
      "d_loss_fake:0.0008888356969691813\n",
      "d_loss_wrong:0.6704885363578796\n",
      "d_loss:0.5992256878089393\n",
      "g_loss:0.32773712277412415\n",
      "Batch:37\n",
      "d_loss_real:0.8732141256332397\n",
      "d_loss_fake:0.0017249415395781398\n",
      "d_loss_wrong:0.685677170753479\n",
      "d_loss:0.6084575908898842\n",
      "g_loss:0.32838210463523865\n",
      "Batch:38\n",
      "d_loss_real:0.8819789886474609\n",
      "d_loss_fake:0.001162177650257945\n",
      "d_loss_wrong:0.68589848279953\n",
      "d_loss:0.6127546594361775\n",
      "g_loss:0.32598572969436646\n",
      "Batch:39\n",
      "d_loss_real:0.8383022546768188\n",
      "d_loss_fake:0.001062644412741065\n",
      "d_loss_wrong:0.7029646039009094\n",
      "d_loss:0.595157939416822\n",
      "g_loss:0.3285025358200073\n",
      "Batch:40\n",
      "d_loss_real:0.8870412707328796\n",
      "d_loss_fake:0.000955925032030791\n",
      "d_loss_wrong:0.6529148817062378\n",
      "d_loss:0.606988337051007\n",
      "g_loss:0.3259502649307251\n",
      "Batch:41\n",
      "d_loss_real:0.8687838315963745\n",
      "d_loss_fake:0.0016022294294089079\n",
      "d_loss_wrong:0.6823060512542725\n",
      "d_loss:0.6053689859691076\n",
      "g_loss:0.33036649227142334\n",
      "Batch:42\n",
      "d_loss_real:0.8685782551765442\n",
      "d_loss_fake:0.0010025706142187119\n",
      "d_loss_wrong:0.6785523295402527\n",
      "d_loss:0.6041778526268899\n",
      "g_loss:0.3258347511291504\n",
      "Batch:43\n",
      "d_loss_real:0.8548243641853333\n",
      "d_loss_fake:0.0010634891223162413\n",
      "d_loss_wrong:0.7131470441818237\n",
      "d_loss:0.6059648154187016\n",
      "g_loss:0.32566800713539124\n",
      "Batch:44\n",
      "d_loss_real:0.8736012578010559\n",
      "d_loss_fake:0.0009191539138555527\n",
      "d_loss_wrong:0.662821352481842\n",
      "d_loss:0.6027357554994524\n",
      "g_loss:0.32575657963752747\n",
      "Batch:45\n",
      "d_loss_real:0.8579462766647339\n",
      "d_loss_fake:0.0008897822699509561\n",
      "d_loss_wrong:0.693163275718689\n",
      "d_loss:0.6024864028295269\n",
      "g_loss:0.3261002004146576\n",
      "Batch:46\n",
      "d_loss_real:0.8789210915565491\n",
      "d_loss_fake:0.0008558528497815132\n",
      "d_loss_wrong:0.6517624855041504\n",
      "d_loss:0.6026151303667575\n",
      "g_loss:0.32673102617263794\n",
      "Batch:47\n",
      "d_loss_real:0.8642656803131104\n",
      "d_loss_fake:0.0008188629290089011\n",
      "d_loss_wrong:0.7022799849510193\n",
      "d_loss:0.6079075521265622\n",
      "g_loss:0.3263964056968689\n",
      "Batch:48\n",
      "d_loss_real:0.8507024049758911\n",
      "d_loss_fake:0.0010465350933372974\n",
      "d_loss_wrong:0.6719132661819458\n",
      "d_loss:0.5935911528067663\n",
      "g_loss:0.32646897435188293\n",
      "Batch:49\n",
      "d_loss_real:0.8677576184272766\n",
      "d_loss_fake:0.0012083055917173624\n",
      "d_loss_wrong:0.6856736540794373\n",
      "d_loss:0.605599299131427\n",
      "g_loss:0.3264347314834595\n",
      "Batch:50\n",
      "d_loss_real:0.8651242256164551\n",
      "d_loss_fake:0.001167932990938425\n",
      "d_loss_wrong:0.684108316898346\n",
      "d_loss:0.6038811752805486\n",
      "g_loss:0.3289634585380554\n",
      "Batch:51\n",
      "d_loss_real:0.8658100366592407\n",
      "d_loss_fake:0.0012775177601724863\n",
      "d_loss_wrong:0.6662757992744446\n",
      "d_loss:0.5997933475882746\n",
      "g_loss:0.3258562684059143\n",
      "Batch:52\n",
      "d_loss_real:0.881976306438446\n",
      "d_loss_fake:0.0010489944834262133\n",
      "d_loss_wrong:0.6730027794837952\n",
      "d_loss:0.6095010967110284\n",
      "g_loss:0.3258170485496521\n",
      "Batch:53\n",
      "d_loss_real:0.8623750805854797\n",
      "d_loss_fake:0.000918853678740561\n",
      "d_loss_wrong:0.6928105354309082\n",
      "d_loss:0.6046198875701521\n",
      "g_loss:0.32765233516693115\n",
      "Batch:54\n",
      "d_loss_real:0.8586285710334778\n",
      "d_loss_fake:0.0010337000712752342\n",
      "d_loss_wrong:0.6773040294647217\n",
      "d_loss:0.5988987179007381\n",
      "g_loss:0.328424870967865\n",
      "Batch:55\n",
      "d_loss_real:0.8581439256668091\n",
      "d_loss_fake:0.0012753799092024565\n",
      "d_loss_wrong:0.6853979229927063\n",
      "d_loss:0.6007402885588817\n",
      "g_loss:0.32709288597106934\n",
      "Batch:56\n",
      "d_loss_real:0.8791207671165466\n",
      "d_loss_fake:0.0015456648543477058\n",
      "d_loss_wrong:0.6627503633499146\n",
      "d_loss:0.6056343906093389\n",
      "g_loss:0.3295937776565552\n",
      "Batch:57\n",
      "d_loss_real:0.8505513072013855\n",
      "d_loss_fake:0.0013777089770883322\n",
      "d_loss_wrong:0.6873871684074402\n",
      "d_loss:0.5974668729468249\n",
      "g_loss:0.3262211084365845\n",
      "Batch:58\n",
      "d_loss_real:0.8604828119277954\n",
      "d_loss_fake:0.00148522958625108\n",
      "d_loss_wrong:0.6737651824951172\n",
      "d_loss:0.5990540089842398\n",
      "g_loss:0.3264862895011902\n",
      "Batch:59\n",
      "d_loss_real:0.8524377346038818\n",
      "d_loss_fake:0.0012898322893306613\n",
      "d_loss_wrong:0.6811290979385376\n",
      "d_loss:0.596823599858908\n",
      "g_loss:0.3281816840171814\n",
      "Batch:60\n",
      "d_loss_real:0.8601230978965759\n",
      "d_loss_fake:0.0012294452171772718\n",
      "d_loss_wrong:0.668079674243927\n",
      "d_loss:0.597388828813564\n",
      "g_loss:0.32829776406288147\n",
      "Batch:61\n",
      "d_loss_real:0.8734946250915527\n",
      "d_loss_fake:0.0016346187330782413\n",
      "d_loss_wrong:0.6822845339775085\n",
      "d_loss:0.6077271007234231\n",
      "g_loss:0.3279295861721039\n",
      "Batch:62\n",
      "d_loss_real:0.8698025941848755\n",
      "d_loss_fake:0.002045938977971673\n",
      "d_loss_wrong:0.6750826835632324\n",
      "d_loss:0.6041834527277388\n",
      "g_loss:0.32664936780929565\n",
      "Batch:63\n",
      "d_loss_real:0.8475149869918823\n",
      "d_loss_fake:0.0010619807289913297\n",
      "d_loss_wrong:0.6908568143844604\n",
      "d_loss:0.5967371922743041\n",
      "g_loss:0.3265845775604248\n",
      "Batch:64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8b5eaf4bdc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# dis 가 실제 이미지를 잘 분류 하게끔 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n\u001b[0;32m---> 93\u001b[0;31m                                                   np.reshape(real_labels, (batch_size, 1)))\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m# gen 이 생성한 가짜 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# dis 가 가짜 이미지를 잘 분류 하게끔 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \"\"\"\n\u001b[1;32m   1668\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   def train_on_batch(self,\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3706\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3707\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    891\u001b[0m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[1;32m    892\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m--> 893\u001b[0;31m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[1;32m    894\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 142\u001b[0;31m         _ctx, \"AssignVariableOp\", name, resource, value)\n\u001b[0m\u001b[1;32m    143\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#with tf.device('/gpu:0'): \n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6 \n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "신경망 빌드 & compile\n",
    "\"\"\"\n",
    "'''\n",
    "ca_model = build_ca_model()\n",
    "ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "'''\n",
    "stage1_dis = build_stage1_discriminator()\n",
    "stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "stage1_gen = build_stage1_generator()\n",
    "stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
    "\n",
    "embedding_compressor_model = build_embedding_compressor_model()\n",
    "embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "'''\n",
    "stage-I GAN 빌드& 컴파일\n",
    "이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "'''\n",
    "adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
    "adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
    "                          optimizer=gen_optimizer, metrics=None)\n",
    "\n",
    "\"\"\"\n",
    "tensorboard.set_model(stage1_gen)\n",
    "tensorboard.set_model(stage1_dis)\n",
    "'''\n",
    "tensorboard.set_model(ca_model)\n",
    "'''\n",
    "tensorboard.set_model(embedding_compressor_model)\n",
    "\"\"\"\n",
    "# Generate an array containing real and fake values\n",
    "# Apply label smoothing as well\n",
    "real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
    "fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
    "'''\n",
    "매 epoch 마다 아래를 반복함\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    print(\"========================================\")\n",
    "    print(\"Epoch is:\", epoch)\n",
    "    print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
    "\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "\n",
    "    # Load data and train model\n",
    "    number_of_batches = int(X_train.shape[0] / batch_size)\n",
    "    for index in range(number_of_batches):\n",
    "        print(\"Batch:{}\".format(index+1))\n",
    "        '''\n",
    "        모델에 입력으로 들어갈 이미지와 텍스트 임베딩을 받아옴 (각 텍스트 임베딩은 각 이미지에 대응 됨)\n",
    "        '''\n",
    "        # 원래는 CA 의 출력이 stage-I 으로 들어가야 하지만 gen 안에 CA 가 있음\n",
    "        z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        # 배치 사이즈만큼 훈련(실제) 이미지를 추출\n",
    "        image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "        # 추출한 이미지에 대응하는 임베딩을 추출\n",
    "        embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
    "        # 이미지들을 정규화하여 값을 작게 만듬\n",
    "        image_batch = (image_batch - 127.5) / 127.5\n",
    "\n",
    "        # stage-I 의 gen 에서 텍스트 임베딩을 바탕으로 저 해상도 fake 이미지를 생성\n",
    "        # 이때, 두 stage 는 랜덤하게 생성한 텍스트 임베딩을 나머지 입력으로 받음\n",
    "        fake_images = stage1_gen.predict(embedding_batch, verbose=3)\n",
    "\n",
    "        # stage-I dis 에 들어갈 압축 텍스트 임베딩을 랜덤하게 생성한 텍스트 임베딩 기반으로 생성\n",
    "        compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
    "        compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
    "        compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
    "        '''\n",
    "        discriminator 에서 입력 이미지가 CNN 을 통과한 결과와 속성값 (40,) 을 concatenate 해 주기 위해서는,\n",
    "        속성값 (40,) 를 compressor 네트워크를 통해 (128,) 로 확장하고,\n",
    "        reshape, tile 함수를 이용해 CNN 결과 (4, 4, 512) 와 (4, 4, :) 부분을 맞추어 주어야 한다.\n",
    "        '''\n",
    "\n",
    "        \"\"\"\n",
    "        stage-I dis 를 훈련함\n",
    "        \"\"\"\n",
    "        # 실제 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 1 (이미지가 진짜라는 의미) 로 하여,\n",
    "        # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n",
    "                                                  np.reshape(real_labels, (batch_size, 1)))\n",
    "        # gen 이 생성한 가짜 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "        # dis 가 가짜 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n",
    "                                                  np.reshape(fake_labels, (batch_size, 1)))\n",
    "        # 실제 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "        # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
    "                                                   np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
    "\n",
    "        d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
    "\n",
    "        print(\"d_loss_real:{}\".format(dis_loss_real))\n",
    "        print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
    "        print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
    "        print(\"d_loss:{}\".format(d_loss))\n",
    "\n",
    "        \"\"\"\n",
    "        stage-I GAN 을 훈련함\n",
    "        이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "        \"\"\"\n",
    "        g_loss = adversarial_model.train_on_batch([embedding_batch, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
    "        print(\"g_loss:{}\".format(g_loss))\n",
    "\n",
    "        dis_losses.append(d_loss)\n",
    "        gen_losses.append(g_loss)\n",
    "\n",
    "    \"\"\"\n",
    "    각 epoch 마다 Tensorboard 에 loss 저장\n",
    "    \"\"\"\n",
    "    writer = tf.summary.create_file_writer(\"logs/\".format(time.time()))\n",
    "    write_log(writer, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
    "    write_log(writer, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "    '''\n",
    "    tf.summary.scalar('discriminator_loss', np.mean(dis_losses), epoch)\n",
    "    tf.summary.scalar('generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "\n",
    "    tf.summary.scalar('discriminator_loss', np.mean(dis_losses))\n",
    "    tf.summary.scalar('generator_loss', np.mean(gen_losses[0]))\n",
    "    summary_op = tf.summary.merge()\n",
    "    summary_writer = tf.summary.FileWriter(\"logs/\".format(time.time()))\n",
    "    '''    \n",
    "    # 매 두번의 epoch 마다 이미지 gen & 이미지 저장\n",
    "    if epoch % 2 == 0:\n",
    "        # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "        z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        embedding_batch = embeddings_test[0:batch_size]\n",
    "        fake_images = stage1_gen.predict_on_batch(embedding_batch)\n",
    "\n",
    "        # Save images\n",
    "        for i, img in enumerate(fake_images[:10]):\n",
    "            save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # Save models\n",
    "        stage1_gen.save_weights(\"weights/stage1_gen_epoch_{}.h5\".format(epoch))\n",
    "        stage1_dis.save_weights(\"weights/stage1_dis_epoch_{}.h5\".format(epoch))\n",
    "\n",
    "'''\n",
    "이제 훈련된 stage-I 의 generator 와 discriminator 을 얻음 (+ embedding_compressor) \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
