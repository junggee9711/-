{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "molecular-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings:  (16000, 40)\n",
      "Embeddings shape: (16000, 40)\n",
      "embeddings:  (4000, 40)\n",
      "Embeddings shape: (4000, 40)\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
    "    concatenate, Flatten, Lambda, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "   \n",
    "def load_class_ids(class_info_file_path):\n",
    "    \"\"\"\n",
    "    Load class ids from class_info.pickle file\n",
    "    \"\"\"\n",
    "    with open(class_info_file_path, 'rb') as f:\n",
    "        class_ids = pickle.load(f, encoding='latin1')\n",
    "        return class_ids\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_file_path):\n",
    "    \"\"\"\n",
    "    훈련된 텍스트 임베딩을 불러옴\n",
    "    \"\"\"\n",
    "    with open(embeddings_file_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f, encoding='latin1')\n",
    "        embeddings = np.array(embeddings)\n",
    "        print('embeddings: ', embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "def load_filenames(filenames_file_path):\n",
    "    \"\"\"\n",
    "    Load filenames.pickle file and return a list of all file names\n",
    "    \"\"\"\n",
    "    with open(filenames_file_path, 'rb') as f:\n",
    "        filenames = pickle.load(f, encoding='latin1')\n",
    "    return filenames\n",
    "\n",
    "def load_bounding_boxes(dataset_dir):\n",
    "    \"\"\"\n",
    "    이미지와 그에 상응하는 바운딩 박스를 짝지어 딕셔너리로 만들어 출력\n",
    "    \"\"\"\n",
    "    # 바운딩 박스 전체 경로\n",
    "    bounding_boxes_path = os.path.join(dataset_dir, 'list_bbox_celeba_pure.csv')\n",
    "    file_paths_path = os.path.join(dataset_dir, 'list_filenames.csv')\n",
    "\n",
    "    # bounding_boxes.txt 와 images.txt 파일을 읽어옴\n",
    "    df_bounding_boxes = pd.read_csv(bounding_boxes_path, header=None).astype(int)\n",
    "    df_file_names = pd.read_csv(file_paths_path, header=None)\n",
    "\n",
    "    # 전체 이미지 파일 명이 순서대로 적힌 리스트를 만듬\n",
    "    file_names = df_file_names[0].tolist() \n",
    "\n",
    "    # 파일 이름에 대응하는 바운딩 박스가 들어갈 딕셔너리를 만듬 (딕셔너리는 크기를 임의로 증가시킬수 있으므로 초기 사이즈는 아무렇게나)\n",
    "    filename_boundingbox_dict = {}\n",
    "\n",
    "    # 이미지 파일과 그에 해당하는 바운딩 박스를 딕셔너리로 만듬 (key = 이미지 파일 이름)\n",
    "    for i in range(0, len(file_names)):\n",
    "        # Get the bounding box\n",
    "        bounding_box = df_bounding_boxes.iloc[i][:].tolist()\n",
    "        key = file_names[i][:-4] + '.jpg'\n",
    "        filename_boundingbox_dict[key] = bounding_box\n",
    "\n",
    "    return filename_boundingbox_dict\n",
    "'''\n",
    "새 이미지가 크롭핑 되어있지 않기 크롭하기 위한 바운딩 박스 좌표 값이 파일에 주어지며,\n",
    "그 파일을 토대로 이미지를 크로핑 한 후,\n",
    "크로핑된 모든 이미지를 지정한 이미지 크기 (image_size) 값으로 바꾼다\n",
    "'''\n",
    "def get_img(img_path, bbox, image_size):\n",
    "    \"\"\"\n",
    "    Load and resize image\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    width, height = img.size\n",
    "    if bbox is not None:\n",
    "        pass\n",
    "    '''\n",
    "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "        y1 = np.maximum(0, center_y - R)\n",
    "        y2 = np.minimum(height, center_y + R)\n",
    "        x1 = np.maximum(0, center_x - R)\n",
    "        x2 = np.minimum(width, center_x + R)\n",
    "        img = img.crop([x1, y1, x2, y2])\n",
    "    '''\n",
    "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_dataset(filenames_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    filenames = load_filenames(filenames_file_path)\n",
    "    '''\n",
    "    class_ids = load_class_ids(class_info_file_path)\n",
    "    '''\n",
    "    bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n",
    "    all_embeddings = load_embeddings(embeddings_file_path)\n",
    "\n",
    "    X, y, embeddings = [], [], []\n",
    "\n",
    "    print(\"Embeddings shape:\", all_embeddings.shape)\n",
    "\n",
    "    # 각 이미지에 해당하는 바운딩 박스 딕셔너리를 추출하여 get_img 함수로 크로핑되고 같은 크기로 바뀐 이미지를 \n",
    "    for index, filename in enumerate(filenames):\n",
    "        bounding_box = bounding_boxes[filename]\n",
    "\n",
    "        try:\n",
    "            # Load images\n",
    "            img_name = '{0}/images/{1}'.format(cub_dataset_dir, filename)\n",
    "            img = get_img(img_name, bounding_box, image_size)\n",
    "            '''\n",
    "            all_embeddings1 = all_embeddings[index, :, :]\n",
    "\n",
    "            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n",
    "            '''\n",
    "            embedding = all_embeddings[index, :]\n",
    "            # X = 정제한 이미지 리스트\n",
    "            X.append(np.array(img))\n",
    "            '''\n",
    "            # y = 정제한 이미지 인덱스\n",
    "            y.append(class_ids[index])\n",
    "            '''\n",
    "            # embeddings = \n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return X, embeddings\n",
    "\n",
    "\n",
    "def generate_c(x):\n",
    "    mean = x[:, :128]\n",
    "    log_sigma = x[:, 128:]\n",
    "    stddev = K.exp(log_sigma)\n",
    "    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n",
    "    c = stddev * epsilon + mean\n",
    "    return c\n",
    "\n",
    "\n",
    "def build_ca_model():\n",
    "    \"\"\"\n",
    "    (1024,)의 텍스트 인코더 신경망의 출력을 입력으로 받고 (256,) 의 텐서를 출력\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(1024,))\n",
    "    x = Dense(256)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    model = Model(inputs=[input_layer], outputs=[x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_embedding_compressor_model():\n",
    "    \"\"\"\n",
    "    입력 속성값 (40,) 을 (128,) 의 벡터로 확장하는 네트워크\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(40,))\n",
    "    x = Dense(10)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=[x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_stage1_generator():\n",
    "    \"\"\"\n",
    "    Stage-I 의 generator \n",
    "    *** 이 신경망 안에 CA 신경망과 생성기 신경망이 들어가 있다!!!! ***\n",
    "    그러므로, 입력으로 텍스트 임베딩 출력 (1024,)과 잡음 변수(100,) 을 받는다\n",
    "    \"\"\"\n",
    "    '''\n",
    "    input_layer = Input(shape=(1024,))\n",
    "    x = Dense(256)(input_layer)\n",
    "    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    c = Lambda(generate_c)(mean_logsigma)\n",
    "\n",
    "    input_layer2 = Input(shape=(100,))\n",
    "    gen_input = Concatenate(axis=1)([c, input_layer2])\n",
    "    '''\n",
    "    # 텍스트 조건부 변수를 잡음 변수와 접합(concatenation) -> cGAN\n",
    "\n",
    "    input_layer = Input(shape=(40,))\n",
    "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(input_layer)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
    "    x = Activation(activation='tanh')(x)\n",
    "\n",
    "    stage1_gen = Model(inputs=input_layer, outputs=x)\n",
    "    '''\n",
    "    stage - I gen 은 입력된 문장의 임베딩을 바탕으로 (+잡음 변수) 이미지를 생성 함 \n",
    "    '''\n",
    "    return stage1_gen\n",
    "\n",
    "\n",
    "def build_stage1_discriminator():\n",
    "    \"\"\"\n",
    "    Create a model which takes two inputs\n",
    "    1. One from above network\n",
    "    2. One from the embedding layer\n",
    "    3. Concatenate along the axis dimension and feed it to the last module which produces final logits\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(64, 64, 3))\n",
    "\n",
    "    x = Conv2D(64, (4, 4),\n",
    "               padding='same', strides=2,\n",
    "               input_shape=(64, 64, 3), use_bias=False)(input_layer)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    '''\n",
    "    실제 이미지에 해당하는 압축된 임베딩을 입력\n",
    "    '''\n",
    "    input_layer2 = Input(shape=(4, 4, 10))\n",
    "\n",
    "    '''\n",
    "    입력 이미지와 압축 텍스트 임베딩을 합침\n",
    "    '''\n",
    "    merged_input = concatenate([x, input_layer2])\n",
    "\n",
    "    x2 = Conv2D(64 * 8, kernel_size=1,\n",
    "                padding=\"same\", strides=1)(merged_input)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    x2 = Dense(1)(x2)\n",
    "    x2 = Activation('sigmoid')(x2)\n",
    "\n",
    "    stage1_dis = Model(inputs=[input_layer, input_layer2], outputs=x2)\n",
    "    '''\n",
    "    출력은 입력 이미지가 진짜인지 가짜인지에 관한 확률(sigmoid)을 출력\n",
    "    '''\n",
    "    return stage1_dis\n",
    "\n",
    "\n",
    "def build_adversarial_model(gen_model, dis_model):\n",
    "    input_layer = Input(shape=(40,))\n",
    "    input_layer3 = Input(shape=(4, 4, 10))\n",
    "\n",
    "    x = gen_model(input_layer)\n",
    "\n",
    "    dis_model.trainable = False\n",
    "    valid = dis_model([x, input_layer3])\n",
    "\n",
    "    model = Model(inputs=[input_layer, input_layer3], outputs=valid)\n",
    "    return model\n",
    "\n",
    "\n",
    "def KL_loss(y_true, y_pred):\n",
    "    mean = y_pred[:, :128]\n",
    "    logsigma = y_pred[:, :128]\n",
    "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
    "    loss = K.mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def custom_generator_loss(y_true, y_pred):\n",
    "    # Calculate binary cross entropy loss\n",
    "    return K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "def save_rgb_img(img, path):\n",
    "    \"\"\"\n",
    "    Save an rgb image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Image\")\n",
    "\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def write_log(callback, name, loss, batch_no):\n",
    "    \"\"\"\n",
    "    Write training summary to TensorBoard\n",
    "    \"\"\"\n",
    "    with callback.as_default():\n",
    "          tf.summary.scalar(name, loss, batch_no)\n",
    "          callback.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Stage - I stackGAN 훈련\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    하이퍼파라미터(불변 파라미터) 지정\n",
    "    '''\n",
    "    data_dir = \"/home/csle/Desktop/CelebA_dataset_reduce\"\n",
    "    train_dir = data_dir + \"/train\"\n",
    "    test_dir = data_dir + \"/test\"\n",
    "    image_size = 64\n",
    "    batch_size = 128\n",
    "    z_dim = 100\n",
    "    stage1_generator_lr = 0.0002\n",
    "    stage1_discriminator_lr = 0.0002\n",
    "    stage1_lr_decay_step = 600\n",
    "    epochs = 1500\n",
    "    condition_dim = 10\n",
    "\n",
    "    embeddings_file_path_train = train_dir + \"/attr_(embeddings).pickle\"\n",
    "    embeddings_file_path_test = test_dir + \"/attr_(embeddings).pickle\"\n",
    "\n",
    "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
    "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
    "\n",
    "    '''\n",
    "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
    "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
    "    '''\n",
    "\n",
    "    cub_dataset_dir = data_dir + \"/img_align_celeba\"\n",
    "\n",
    "    '''\n",
    "    optimizer 정의\n",
    "    '''\n",
    "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    \"\"\"\"\n",
    "    dataset 로드하기\n",
    "    \"\"\"\n",
    "    X_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n",
    "                                                      cub_dataset_dir=cub_dataset_dir,\n",
    "                                                      embeddings_file_path=embeddings_file_path_train,\n",
    "                                                      image_size=(64, 64))\n",
    "\n",
    "    X_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n",
    "                                                   cub_dataset_dir=cub_dataset_dir,\n",
    "                                                   embeddings_file_path=embeddings_file_path_test,\n",
    "                                                   image_size=(64, 64))\n",
    "    \n",
    "    print('finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Epoch is: 0\n",
      "Number of batches 125\n",
      "Batch:1\n",
      "d_loss_real:0.47643357515335083\n",
      "d_loss_fake:2.9947659969329834\n",
      "d_loss:2.949436232447624\n",
      "g_loss:0.7523037195205688\n",
      "Batch:2\n",
      "d_loss_real:4.6329240798950195\n",
      "d_loss_fake:1.778754472732544\n",
      "d_loss:1.4109269948676229\n",
      "g_loss:0.8690600991249084\n",
      "Batch:3\n",
      "d_loss_real:4.15828800201416\n",
      "d_loss_fake:0.034253690391778946\n",
      "d_loss:1.439252254087478\n",
      "g_loss:0.7414202094078064\n",
      "Batch:4\n",
      "d_loss_real:3.172417640686035\n",
      "d_loss_fake:1.2280186414718628\n",
      "d_loss:1.0944245681166649\n",
      "g_loss:0.8933128714561462\n",
      "Batch:5\n",
      "d_loss_real:3.694983959197998\n",
      "d_loss_fake:0.010391928255558014\n",
      "d_loss:1.2498221369460225\n",
      "g_loss:0.7687890529632568\n",
      "Batch:6\n",
      "d_loss_real:3.0551564693450928\n",
      "d_loss_fake:1.636728286743164\n",
      "d_loss:1.1137026958167553\n",
      "g_loss:0.9433993697166443\n",
      "Batch:7\n",
      "d_loss_real:3.2931439876556396\n",
      "d_loss_fake:0.002671379130333662\n",
      "d_loss:1.1043331422260962\n",
      "g_loss:0.7845420837402344\n",
      "Batch:8\n",
      "d_loss_real:2.487905502319336\n",
      "d_loss_fake:0.491138219833374\n",
      "d_loss:0.891637809574604\n",
      "g_loss:0.8424414396286011\n",
      "Batch:9\n",
      "d_loss_real:2.6772098541259766\n",
      "d_loss_fake:0.002637340920045972\n",
      "d_loss:0.9507949817052577\n",
      "g_loss:0.7545564770698547\n",
      "Batch:10\n",
      "d_loss_real:2.491720676422119\n",
      "d_loss_fake:0.08050983399152756\n",
      "d_loss:0.8594599952921271\n",
      "g_loss:0.7420745491981506\n",
      "Batch:11\n",
      "d_loss_real:2.3042731285095215\n",
      "d_loss_fake:0.013571484945714474\n",
      "d_loss:0.8558259065030143\n",
      "g_loss:0.7045049667358398\n",
      "Batch:12\n",
      "d_loss_real:2.255361795425415\n",
      "d_loss_fake:0.01888936012983322\n",
      "d_loss:0.8235886534675956\n",
      "g_loss:0.6643694639205933\n",
      "Batch:13\n",
      "d_loss_real:2.130155563354492\n",
      "d_loss_fake:0.019277606159448624\n",
      "d_loss:0.7956716832704842\n",
      "g_loss:0.6276336908340454\n",
      "Batch:14\n",
      "d_loss_real:2.0774154663085938\n",
      "d_loss_fake:0.011163990944623947\n",
      "d_loss:0.7721640518866479\n",
      "g_loss:0.5901020765304565\n",
      "Batch:15\n",
      "d_loss_real:2.1088788509368896\n",
      "d_loss_fake:0.015439886599779129\n",
      "d_loss:0.7915935763157904\n",
      "g_loss:0.5558859705924988\n",
      "Batch:16\n",
      "d_loss_real:1.9960554838180542\n",
      "d_loss_fake:0.009147156029939651\n",
      "d_loss:0.748462772462517\n",
      "g_loss:0.5314731001853943\n",
      "Batch:17\n",
      "d_loss_real:1.9349883794784546\n",
      "d_loss_fake:0.011553049087524414\n",
      "d_loss:0.7425922676920891\n",
      "g_loss:0.5047452449798584\n",
      "Batch:18\n",
      "d_loss_real:1.907869815826416\n",
      "d_loss_fake:0.00788145698606968\n",
      "d_loss:0.7449983127880841\n",
      "g_loss:0.48444342613220215\n",
      "Batch:19\n",
      "d_loss_real:1.870316505432129\n",
      "d_loss_fake:0.017402570694684982\n",
      "d_loss:0.7206692821346223\n",
      "g_loss:0.4683886170387268\n",
      "Batch:20\n",
      "d_loss_real:1.8981339931488037\n",
      "d_loss_fake:0.004087273497134447\n",
      "d_loss:0.7220836361520924\n",
      "g_loss:0.4427812099456787\n",
      "Batch:21\n",
      "d_loss_real:1.7667157649993896\n",
      "d_loss_fake:0.004160059150308371\n",
      "d_loss:0.7116301596979611\n",
      "g_loss:0.4201294183731079\n",
      "Batch:22\n",
      "d_loss_real:1.9201444387435913\n",
      "d_loss_fake:0.005181972868740559\n",
      "d_loss:0.7109508930006996\n",
      "g_loss:0.3931347727775574\n",
      "Batch:23\n",
      "d_loss_real:1.662286400794983\n",
      "d_loss_fake:0.0034250300377607346\n",
      "d_loss:0.6874483560677618\n",
      "g_loss:0.38124364614486694\n",
      "Batch:24\n",
      "d_loss_real:1.716831922531128\n",
      "d_loss_fake:0.004381765145808458\n",
      "d_loss:0.6822596700512804\n",
      "g_loss:0.371662974357605\n",
      "Batch:25\n",
      "d_loss_real:1.704539179801941\n",
      "d_loss_fake:0.004527226090431213\n",
      "d_loss:0.6887988913804293\n",
      "g_loss:0.35961973667144775\n",
      "Batch:26\n",
      "d_loss_real:1.7165958881378174\n",
      "d_loss_fake:0.002280101180076599\n",
      "d_loss:0.6805553566664457\n",
      "g_loss:0.3493286967277527\n",
      "Batch:27\n",
      "d_loss_real:1.7446686029434204\n",
      "d_loss_fake:0.0027195224538445473\n",
      "d_loss:0.6826905311318114\n",
      "g_loss:0.3425878882408142\n",
      "Batch:28\n",
      "d_loss_real:1.709404468536377\n",
      "d_loss_fake:0.00309467944316566\n",
      "d_loss:0.6817191847076174\n",
      "g_loss:0.33737480640411377\n",
      "Batch:29\n",
      "d_loss_real:1.6474136114120483\n",
      "d_loss_fake:0.002637480851262808\n",
      "d_loss:0.6672666861559264\n",
      "g_loss:0.3321792483329773\n",
      "Batch:30\n",
      "d_loss_real:1.6472015380859375\n",
      "d_loss_fake:0.002151409862563014\n",
      "d_loss:0.6696156387624796\n",
      "g_loss:0.3328993022441864\n",
      "Batch:31\n",
      "d_loss_real:1.6245973110198975\n",
      "d_loss_fake:0.002161765471100807\n",
      "d_loss:0.6529949561227113\n",
      "g_loss:0.3296533524990082\n",
      "Batch:32\n",
      "d_loss_real:1.6799737215042114\n",
      "d_loss_fake:0.0020886564161628485\n",
      "d_loss:0.6616467002022546\n",
      "g_loss:0.3284150958061218\n",
      "Batch:33\n",
      "d_loss_real:1.5312919616699219\n",
      "d_loss_fake:0.0019330712966620922\n",
      "d_loss:0.6469045322737657\n",
      "g_loss:0.3277287483215332\n",
      "Batch:34\n",
      "d_loss_real:1.5991039276123047\n",
      "d_loss_fake:0.0020636687986552715\n",
      "d_loss:0.6477101342170499\n",
      "g_loss:0.3267292380332947\n",
      "Batch:35\n",
      "d_loss_real:1.55220627784729\n",
      "d_loss_fake:0.0019068120745941997\n",
      "d_loss:0.6500310301635182\n",
      "g_loss:0.32729971408843994\n",
      "Batch:36\n",
      "d_loss_real:1.5942397117614746\n",
      "d_loss_fake:0.001542172976769507\n",
      "d_loss:0.6446201158250915\n",
      "g_loss:0.3260493576526642\n",
      "Batch:37\n",
      "d_loss_real:1.6142868995666504\n",
      "d_loss_fake:0.0015476085245609283\n",
      "d_loss:0.6614529830403626\n",
      "g_loss:0.3278443217277527\n",
      "Batch:38\n",
      "d_loss_real:1.5393813848495483\n",
      "d_loss_fake:0.0014939383836463094\n",
      "d_loss:0.6373546729009831\n",
      "g_loss:0.3260696530342102\n",
      "Batch:39\n",
      "d_loss_real:1.6132762432098389\n",
      "d_loss_fake:0.0016435126308351755\n",
      "d_loss:0.646494221437024\n",
      "g_loss:0.3262881636619568\n",
      "Batch:40\n",
      "d_loss_real:1.6127002239227295\n",
      "d_loss_fake:0.0018832876812666655\n",
      "d_loss:0.6426938423828688\n",
      "g_loss:0.3255072832107544\n",
      "Batch:41\n",
      "d_loss_real:1.509505271911621\n",
      "d_loss_fake:0.0018228443805128336\n",
      "d_loss:0.6354935411654878\n",
      "g_loss:0.3289704918861389\n",
      "Batch:42\n",
      "d_loss_real:1.5164148807525635\n",
      "d_loss_fake:0.0019177824724465609\n",
      "d_loss:0.627234027168015\n",
      "g_loss:0.32538843154907227\n",
      "Batch:43\n",
      "d_loss_real:1.5021313428878784\n",
      "d_loss_fake:0.0016692576464265585\n",
      "d_loss:0.6326571814424824\n",
      "g_loss:0.3254111707210541\n",
      "Batch:44\n",
      "d_loss_real:1.5592460632324219\n",
      "d_loss_fake:0.0019031157717108727\n",
      "d_loss:0.6289726047543809\n",
      "g_loss:0.32539665699005127\n",
      "Batch:45\n",
      "d_loss_real:1.5408672094345093\n",
      "d_loss_fake:0.0017506503500044346\n",
      "d_loss:0.6349700170685537\n",
      "g_loss:0.325660765171051\n",
      "Batch:46\n",
      "d_loss_real:1.5277619361877441\n",
      "d_loss_fake:0.0018410359043627977\n",
      "d_loss:0.6243837137299124\n",
      "g_loss:0.3268846571445465\n",
      "Batch:47\n",
      "d_loss_real:1.4626015424728394\n",
      "d_loss_fake:0.0022344188764691353\n",
      "d_loss:0.6265410581836477\n",
      "g_loss:0.32529351115226746\n",
      "Batch:48\n",
      "d_loss_real:1.5144727230072021\n",
      "d_loss_fake:0.002314778510481119\n",
      "d_loss:0.6170530992676504\n",
      "g_loss:0.3256637454032898\n",
      "Batch:49\n",
      "d_loss_real:1.489437460899353\n",
      "d_loss_fake:0.002238269429653883\n",
      "d_loss:0.6251173237687908\n",
      "g_loss:0.32517439126968384\n",
      "Batch:50\n",
      "d_loss_real:1.5129144191741943\n",
      "d_loss_fake:0.0017210760852321982\n",
      "d_loss:0.6302276318165241\n",
      "g_loss:0.3254793584346771\n",
      "Batch:51\n",
      "d_loss_real:1.5316983461380005\n",
      "d_loss_fake:0.0022759055718779564\n",
      "d_loss:0.619873444433324\n",
      "g_loss:0.325204074382782\n",
      "Batch:52\n",
      "d_loss_real:1.4769675731658936\n",
      "d_loss_fake:0.002031446900218725\n",
      "d_loss:0.617212139244657\n",
      "g_loss:0.32534706592559814\n",
      "Batch:53\n",
      "d_loss_real:1.4450578689575195\n",
      "d_loss_fake:0.0019501345232129097\n",
      "d_loss:0.619140817434527\n",
      "g_loss:0.32520535588264465\n",
      "Batch:54\n",
      "d_loss_real:1.495298147201538\n",
      "d_loss_fake:0.0016849283128976822\n",
      "d_loss:0.6145694733131677\n",
      "g_loss:0.32545337080955505\n",
      "Batch:55\n",
      "d_loss_real:1.474454402923584\n",
      "d_loss_fake:0.0022242304403334856\n",
      "d_loss:0.6126686214993242\n",
      "g_loss:0.3261110484600067\n",
      "Batch:56\n",
      "d_loss_real:1.4802906513214111\n",
      "d_loss_fake:0.0017547447932884097\n",
      "d_loss:0.6121655608149013\n",
      "g_loss:0.32728517055511475\n",
      "Batch:57\n",
      "d_loss_real:1.4220614433288574\n",
      "d_loss_fake:0.002321422565728426\n",
      "d_loss:0.6122771502123214\n",
      "g_loss:0.32519984245300293\n",
      "Batch:58\n",
      "d_loss_real:1.4809083938598633\n",
      "d_loss_fake:0.0021262296941131353\n",
      "d_loss:0.6100235485646408\n",
      "g_loss:0.3255777955055237\n",
      "Batch:59\n",
      "d_loss_real:1.4556766748428345\n",
      "d_loss_fake:0.0018471106886863708\n",
      "d_loss:0.6084362482652068\n",
      "g_loss:0.32529404759407043\n",
      "Batch:60\n",
      "d_loss_real:1.4496643543243408\n",
      "d_loss_fake:0.0016852424014359713\n",
      "d_loss:0.6012196293158922\n",
      "g_loss:0.3252573013305664\n",
      "Batch:61\n",
      "d_loss_real:1.4413350820541382\n",
      "d_loss_fake:0.0019209020538255572\n",
      "d_loss:0.6197210132231703\n",
      "g_loss:0.3252963125705719\n",
      "Batch:62\n",
      "d_loss_real:1.4816423654556274\n",
      "d_loss_fake:0.0016229358734562993\n",
      "d_loss:0.6095051828451687\n",
      "g_loss:0.32528528571128845\n",
      "Batch:63\n",
      "d_loss_real:1.424512505531311\n",
      "d_loss_fake:0.0015361398691311479\n",
      "d_loss:0.6056407365977066\n",
      "g_loss:0.3251532316207886\n",
      "Batch:64\n",
      "d_loss_real:1.4540399312973022\n",
      "d_loss_fake:0.0017159241251647472\n",
      "d_loss:0.6039871530956589\n",
      "g_loss:0.32535284757614136\n",
      "Batch:65\n",
      "d_loss_real:1.444345235824585\n",
      "d_loss_fake:0.0013520518550649285\n",
      "d_loss:0.6050837722868891\n",
      "g_loss:0.32526034116744995\n",
      "Batch:66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_real:1.45143461227417\n",
      "d_loss_fake:0.0014218536671251059\n",
      "d_loss:0.6082753371156286\n",
      "g_loss:0.3256886601448059\n",
      "Batch:67\n",
      "d_loss_real:1.4014971256256104\n",
      "d_loss_fake:0.0013359661679714918\n",
      "d_loss:0.5939034692419227\n",
      "g_loss:0.32537031173706055\n",
      "Batch:68\n"
     ]
    }
   ],
   "source": [
    "#with tf.device('/gpu:0'): \n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6 \n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "신경망 빌드 & compile\n",
    "\"\"\"\n",
    "'''\n",
    "ca_model = build_ca_model()\n",
    "ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "'''\n",
    "stage1_dis = build_stage1_discriminator()\n",
    "stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "stage1_gen = build_stage1_generator()\n",
    "stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
    "\n",
    "embedding_compressor_model = build_embedding_compressor_model()\n",
    "embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "'''\n",
    "stage-I GAN 빌드& 컴파일\n",
    "이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "'''\n",
    "adversarial_model = build_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
    "adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0],\n",
    "                          optimizer=gen_optimizer, metrics=None)\n",
    "\n",
    "\"\"\"\n",
    "tensorboard.set_model(stage1_gen)\n",
    "tensorboard.set_model(stage1_dis)\n",
    "'''\n",
    "tensorboard.set_model(ca_model)\n",
    "'''\n",
    "tensorboard.set_model(embedding_compressor_model)\n",
    "\"\"\"\n",
    "# Generate an array containing real and fake values\n",
    "# Apply label smoothing as well\n",
    "real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
    "fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
    "'''\n",
    "매 epoch 마다 아래를 반복함\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    print(\"========================================\")\n",
    "    print(\"Epoch is:\", epoch)\n",
    "    print(\"Number of batches\", int(X_train.shape[0] / batch_size))\n",
    "\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "\n",
    "    # Load data and train model\n",
    "    number_of_batches = int(X_train.shape[0] / batch_size)\n",
    "    for index in range(number_of_batches):\n",
    "        print(\"Batch:{}\".format(index+1))\n",
    "        '''\n",
    "        모델에 입력으로 들어갈 이미지와 텍스트 임베딩을 받아옴 (각 텍스트 임베딩은 각 이미지에 대응 됨)\n",
    "        '''\n",
    "        # 원래는 CA 의 출력이 stage-I 으로 들어가야 하지만 gen 안에 CA 가 있음\n",
    "        z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        # 배치 사이즈만큼 훈련(실제) 이미지를 추출\n",
    "        image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "        # 추출한 이미지에 대응하는 임베딩을 추출\n",
    "        embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n",
    "        # 이미지들을 정규화하여 값을 작게 만듬\n",
    "        image_batch = (image_batch - 127.5) / 127.5\n",
    "\n",
    "        # stage-I 의 gen 에서 텍스트 임베딩을 바탕으로 저 해상도 fake 이미지를 생성\n",
    "        # 이때, 두 stage 는 랜덤하게 생성한 텍스트 임베딩을 나머지 입력으로 받음\n",
    "        fake_images = stage1_gen.predict(embedding_batch, verbose=3)\n",
    "\n",
    "        # stage-I dis 에 들어갈 압축 텍스트 임베딩을 랜덤하게 생성한 텍스트 임베딩 기반으로 생성\n",
    "        compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
    "        compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
    "        compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
    "        '''\n",
    "        discriminator 에서 입력 이미지가 CNN 을 통과한 결과와 속성값 (40,) 을 concatenate 해 주기 위해서는,\n",
    "        속성값 (40,) 를 compressor 네트워크를 통해 (128,) 로 확장하고,\n",
    "        reshape, tile 함수를 이용해 CNN 결과 (4, 4, 512) 와 (4, 4, :) 부분을 맞추어 주어야 한다.\n",
    "        '''\n",
    "\n",
    "        \"\"\"\n",
    "        stage-I dis 를 훈련함\n",
    "        \"\"\"\n",
    "        # 실제 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 1 (이미지가 진짜라는 의미) 로 하여,\n",
    "        # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding],\n",
    "                                                  np.reshape(real_labels, (batch_size, 1)))\n",
    "        # gen 이 생성한 가짜 이미지와 압축 텍스트 임베딩을 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "        # dis 가 가짜 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding],\n",
    "                                                  np.reshape(fake_labels, (batch_size, 1)))\n",
    "        # 실제 이미지와 압축 텍스트 임베딩을 '어긋나게' 입력으로 하고 모든 레이블을 0 (이미지가 가짜라는 의미) 로 하여,\n",
    "        # dis 가 실제 이미지를 잘 분류 하게끔 훈련\n",
    "        dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size - 1)], compressed_embedding[1:]],\n",
    "                                                   np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
    "        dis_loss_wrong2 = stage1_dis.train_on_batch([image_batch[:(batch_size - 2)], compressed_embedding[2:]],\n",
    "                                                   np.reshape(fake_labels[2:], (batch_size-2, 1)))\n",
    "        \n",
    "        d_loss = 0.5*np.add(dis_loss_wrong2, 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake)))\n",
    "        \n",
    "        print(\"d_loss_real:{}\".format(dis_loss_real))\n",
    "        print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
    "        '''\n",
    "        print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
    "        '''\n",
    "        print(\"d_loss:{}\".format(d_loss))\n",
    "\n",
    "        \"\"\"\n",
    "        stage-I GAN 을 훈련함\n",
    "        이때, stage-I 의 discriminator 는 훈련시키지 않고 stage-I generator 의 가중치만 업데이트\n",
    "        \"\"\"\n",
    "        g_loss = adversarial_model.train_on_batch([embedding_batch, compressed_embedding],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
    "        print(\"g_loss:{}\".format(g_loss))\n",
    "\n",
    "        dis_losses.append(d_loss)\n",
    "        gen_losses.append(g_loss)\n",
    "\n",
    "    \"\"\"\n",
    "    각 epoch 마다 Tensorboard 에 loss 의 전체 배치 평균값 저장\n",
    "    \"\"\"\n",
    "    writer = tf.summary.create_file_writer(\"logs/\".format(time.time()))\n",
    "    write_log(writer, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
    "    write_log(writer, 'generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "    '''\n",
    "    tf.summary.scalar('discriminator_loss', np.mean(dis_losses), epoch)\n",
    "    tf.summary.scalar('generator_loss', np.mean(gen_losses[0]), epoch)\n",
    "\n",
    "    tf.summary.scalar('discriminator_loss', np.mean(dis_losses))\n",
    "    tf.summary.scalar('generator_loss', np.mean(gen_losses[0]))\n",
    "    summary_op = tf.summary.merge()\n",
    "    summary_writer = tf.summary.FileWriter(\"logs/\".format(time.time()))\n",
    "    '''    \n",
    "    # 매 두번의 epoch 마다 이미지 gen & 이미지 저장\n",
    "    if epoch % 2 == 0:\n",
    "        # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "        z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        embedding_batch = embeddings_test[0:batch_size]\n",
    "        fake_images = stage1_gen.predict_on_batch(embedding_batch)\n",
    "\n",
    "        # Save images\n",
    "        for i, img in enumerate(fake_images[:10]):\n",
    "            save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # Save models\n",
    "        stage1_gen.save_weights(\"weights/stage1_gen_epoch_{}.h5\".format(epoch))\n",
    "        stage1_dis.save_weights(\"weights/stage1_dis_epoch_{}.h5\".format(epoch))\n",
    "\n",
    "'''\n",
    "이제 훈련된 stage-I 의 generator 와 discriminator 을 얻음 (+ embedding_compressor) \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
