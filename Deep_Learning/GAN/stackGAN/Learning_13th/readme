1. build_stage1_generator 에서 CA 는 통과하지 않지만, mean_logsigma 는 출력하여 build_adversarial_model 에 mean_logsigma 가 있어야 한다
 
 --> 왜?) 이 값이 adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0], optimizer=gen_optimizer, metrics=None) 
          에서, KL_loss 를 구하는 입력으로 들어가고 g_loss = 1*binary_crossentropy + 2*KL_loss 가 되어 stackGAN 의 손실함수와 같아 질 것이기 때문
 
 --> loss function 이 g_loss = 1*binary_crossentropy + 2*KL_loss 임
 
 --> 내가 없애도 되는 것은 (100,) 인 noise 뿐인 것
 
 --> 1) def build_stage1_generator(): --> 로 바꿈
            input_layer = Input(shape=(40,))
            x = Dense(10)(input_layer)
            mean_logsigma = LeakyReLU(alpha=0.2)(x)

            x = Dense(128 * 8 * 4 * 4, use_bias=False)(input_layer)
            x = ReLU()(x)

            x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)
            x = Dropout(0.0)(x)

            x = UpSampling2D(size=(2, 2))(x)
            x = Conv2D(512, kernel_size=3, padding="same", strides=1, use_bias=False)(x)
            x = BatchNormalization()(x)
            x = ReLU()(x)

            x = UpSampling2D(size=(2, 2))(x)
            x = Conv2D(256, kernel_size=3, padding="same", strides=1, use_bias=False)(x)
            x = BatchNormalization()(x)
            x = ReLU()(x)

            x = UpSampling2D(size=(2, 2))(x)
            x = Conv2D(128, kernel_size=3, padding="same", strides=1, use_bias=False)(x)
            x = BatchNormalization()(x)
            x = ReLU()(x)

            x = UpSampling2D(size=(2, 2))(x)
            x = Conv2D(64, kernel_size=3, padding="same", strides=1, use_bias=False)(x)
            x = BatchNormalization()(x)
            x = ReLU()(x)

            x = Conv2D(3, kernel_size=3, padding="same", strides=1, use_bias=False)(x)
            x = Activation(activation='tanh')(x)

            stage1_gen = Model(inputs=input_layer, outputs=[x, mean_logsigma])
            '''
            stage - I gen 은 입력된 문장의 임베딩을 바탕으로 (+잡음 변수) 이미지를 생성 함 
            '''
            return stage1_gen

     2) def build_adversarial_model(gen_model, dis_model): --> 로 바꿈
            input_layer = Input(shape=(40,))
            input_layer3 = Input(shape=(4, 4, 10))

            x, mean_logsigma = gen_model(input_layer)

            dis_model.trainable = False
            valid = dis_model([x, input_layer3])

            model = Model(inputs=[input_layer, input_layer3], outputs=[valid, mean_logsigma)
            return model     
     
     3) stage1_gen.predict_on_batch 의 출력이 1개에서 2개가 되었으므로 , _ 추가
 
 --> g_loss 가 [0] 에서 [0,1,2] 로 나오는 것은 
     g_loss[0] = 1*binary_crossentropy + 2*KL_loss
     g_loss[1] = binary_crossentropy
     g_loss[2] = KL_loss
     이기 떄문임
     그러므로, g_loss[0] 이 우리가 원하는 g_loss 값임
     
2.  stage1_dis = build_stage1_discriminator()
    stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer) 
     --> loss function 이 binary_crossentropy 임
     
    stage1_gen = build_stage1_generator()
    stage1_gen.compile(loss="mse", optimizer=gen_optimizer)
     --> loss function 이 mse(평균제곱오차) 임

3. 혹시 모르니,
   ca_model = build_ca_model()
   ca_model.compile(loss="binary_crossentropy", optimizer="adam") 도 compile
    --> build_stage1_generator 안에서 Lambda(generate_c) 의 형태로 generate_c 함수가 keras 레이어로 불려오게 되는 것이라,
        난 사용을 하지 않지만, 혹시 모르니까 compile
        
